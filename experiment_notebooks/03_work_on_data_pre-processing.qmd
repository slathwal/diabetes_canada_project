---
title: "Working on data pre-processing for diabetes prediction"
author: "Shefali Lathwal"
date: "2025-05-19"
date-modified: last-modified
toc: true
format: html
jupyter: python3
echo: true
---

# Import required libraries
```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score
from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from xgboost import XGBClassifier
from sklearn.metrics import ConfusionMatrixDisplay, precision_recall_curve, average_precision_score
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.base import clone
import seaborn as sns
from sklearn.feature_selection import VarianceThreshold
from lightgbm import LGBMClassifier
from sklearn.manifold import TSNE
import shap
from sklearn.model_selection import learning_curve
from IPython.display import Markdown
```

# 1. Get the data
```{python}
df = pd.read_csv("/Users/shefalilathwal/Documents/diabetes_canada_project/data/pumf_cchs.csv")
print(df.shape)
col_list = df.columns.tolist()
print(col_list)

# Only keeping rows with a known diabetes status
#Codes 9, 7, and 8 are for either refusing to disclose diabetes status, or not knowing or not stated. I should only keep the rows with known diabetes status.

df = df[df["CCC_095"].isin([1.0, 2.0])]
print(df["CCC_095"].value_counts())

# Only keeping rows for adults - removing code = 1
df = df[~(df["DHHGAGE"] == 1)]
print(df["DHHGAGE"].value_counts())

print(df.shape)
```

# 2. Sample a test set, put it aside, and never look at it (no data snooping).
```{python}
df_train_set, df_test_set = train_test_split(df, test_size = 0.3, random_state = 42, shuffle = True, stratify = df["CCC_095"])

## Confirm stratification
print(df_train_set["CCC_095"].value_counts()/df_train_set.shape[0], df_test_set["CCC_095"].value_counts()/df_test_set.shape[0], df["CCC_095"].value_counts()/df.shape[0])

X_train = df_train_set.copy().drop(columns = "CCC_095", inplace = False)
y_train = df_train_set["CCC_095"].copy()

X_test = df_test_set.copy().drop(columns = "CCC_095", inplace = False)
y_test = df_test_set["CCC_095"].copy()


# Convert the target variable into True and False
y_train = (y_train == 1)
y_test = (y_test == 1)
y_train.sum(), y_test.sum()
```

# 3. Explore the data

## 3. Study each attribute and its characteristics:
- Name
- Type (Cateogorical - int/flot, bounded/unbounded, test, structured etc.)
- % of missing values
- Noisiness and type of noise (stochastic, outliers, rounding errors, etc.)
- usefulness for the task
- type of distribution (Gaussian, uniform, logarithmic etc.)

I studied the attributes in previous notebooks. I need the following steps:

- Calculate % of missing values and drop features with a lot of missing values
- Study correlation between attributes and drop attributes that are highly correlated.
- Visualize remaining features


### Define a function to calculate missing values and remove columns with a large number of missing data
```{python}
# Step1:  convert_missing_codes to na

def check_special_missing(df):
    df_cleaned = df.copy()
    missing_summary = {}
    #max_val_list = []
    for col in df.columns:
        #print(col)
        if df[col].dtype in [np.float64, np.int64]:
            unique_vals = df[col].dropna().unique()
            #print(len(unique_vals), unique_vals)
            if len(unique_vals) == 0:
                continue
            max_val = max(unique_vals)
            #max_val_list.append(max_val)
            #print(max_val)
            missing_codes = []
            # Identify the missing value codes based on magnitude
            if max_val < 10:  # single digits (1–9)
                missing_codes = [6, 7, 8, 9]
            elif max_val < 100:  # double digits (01–99)
                missing_codes = [96, 97, 98, 99]
            elif max_val < 1000:  # triple digits (001–999)
                missing_codes = [996, 997, 998, 999, 999.6, 999.7, 999.8, 999.9]
            elif max_val < 10000:
                missing_codes = [9996, 9997, 9998, 9999, 9999.90, 9999.80, 9999.70, 9999.60]
            elif max_val < 100000:
                missing_codes = [99996, 99997, 99998, 99999]
            else:
                continue
            
            # Count and replace
            if missing_codes:
                counts = {code: (df[col] == code).sum() for code in missing_codes}
                if any(counts.values()):
                    missing_summary[col] = counts
                df_cleaned[col] = df_cleaned[col].replace(missing_codes, np.nan)

    # Return cleaned data and summary
    return df_cleaned

df_nan = check_special_missing(X_train)
```

```{python}
cols_missing_data = pd.Series(round(df_nan.isna().sum()/df_nan.shape[0]*100,2))
#cols_missing_data

fig, ax = plt.subplots()
cols_missing_data.plot.hist(bins = 20, ax = ax)
ax.set_xlabel("Percentage of data missing")
ax.set_title("Histogram of percentage of data missing across all columns")
print(f" Out of a total {cols_missing_data.shape[0]}, there are {(cols_missing_data >= 30).sum()} columns with more than 30% data missing.")
```

Subset the data to include only columns with less than 30% data missing
```{python}
X_train = X_train.loc[:, ~(cols_missing_data >= 30)]
X_test = X_test.loc[:, ~(cols_missing_data >= 30)]
```

Remove some additional columns that we know are not likely to be related to diabetes status

```{python}
cols_to_exclude = ['ADM_RNO1', 'VERDATE', 'REFPER', 'ADM_PRX','GEOGPRV', 'GEODGHR4', 'ADM_045','WTS_M']
X_train = X_train.drop(columns = cols_to_exclude, inplace = False)
X_test = X_test.drop(columns = cols_to_exclude, inplace = False)
```

Remove columns that have the same value across all rows
```{python}
df_nan = check_special_missing(X_train)
selector = VarianceThreshold()
selector.fit(df_nan)
selector.get_support()
#selector.variances_
non_zero_variance_cols = df_nan.columns[~selector.get_support()]
non_zero_variance_cols
X_train = X_train.drop(columns = non_zero_variance_cols, inplace = False)
X_test = X_test.drop(columns = non_zero_variance_cols, inplace = False)
```

Remove other columns starting with the string "DO" because these are columns that just contain a survey response flag.
Some examples include: ['DOMAC', 'DOGEN','DOHWT','DOCCC', 'DOHUI','DOCIH', 'DOFVC', 'DOFGU', 'DOSMK', 'DOTAL', 'DOETS', 'DOALC', 'DOALW', 'DOAMU', 'DOCAN', 'DOSD,S', 'DODRG', 'DOPAA', 'DOPAY', 'DOSBE', 'DOSXB', 'DODRV', 'DODWI', 'DOFLU', 'DOBPC', 'DOMAM', 'DOCCT','DOCMH','DOSWL', 'DODEP', 'DOSUI','DOSPS','DOPHC', 'DOMDA', 'DOCP2','DOCP3','DOPNC','DOPSC', 'DOPEX','DOUCN','DOSDC', 'DOPMK', 'DOINS','DOFSC']

```{python}
X_train = X_train.loc[:,~X_train.columns.str.startswith('DO')]
X_test = X_test.loc[:,~X_test.columns.str.startswith('DO')]
```


### Study correlation between attributes and drop attributes that are highly correlated.
In order to study correlation, I will have to identify which features are numerical, which are categorical and which are ordinal. To do the above, I will need to look at the distribution of each variable.
First check how many columns we have
```{python}
print(f"Number of columns so far: {X_train.shape[1]}")
```

```{python}
df_nan = check_special_missing(X_train)
for col in X_train.columns:
    fig, ax = plt.subplots()
    df_nan[col].hist(ax = ax)
    ax.set_title(col)
    plt.show()
    plt.close(fig)
```

None of the columns included at this stage contain continuous values. However, some columns are likely ordinal where increasing codes are meaningful. Should I separate ordinal variables from categorical?

Since my features are categorical, pearson is not the right metric to use.
- I should use spearman rank correlation for ordinal variables
- For categorical variables, chatGPT is suggesting something else:
```{python}
ord_cols = ["INCDGRRS", "INCDGRPR", "INCDGRCA", "INCDGHH", "FSCDVHF2","FSCDVAF2", "PNCDVNED", "PNCDVPNO", "PNCDVPNC", "PNCDVPNM", "PNCDVPNI", "PNCDVHCT", "PHC_035", "ALCDVTTM","ALC_020", "ALC_015", "SMKDVSTY", "SMK_005","GENDVSWL","GENDVMHI","GENDVHDI","GEN_030","GEN_020","GEN_015","GEN_010","GEN_005","EHG2DVH3","DHHGAGE"]

cat_cols = [col for col in X_train.columns if col not in ord_cols]
#print(cat_cols)
len(cat_cols) + len(ord_cols) == len(X_train.columns)
```

```{python}
from scipy.stats import chi2_contingency

# I will work with dataframe containing nan values for calculating correlations, otherwise the correlations would be incorrect.


# For ordinal variables
spearman_corr = df_nan[ord_cols].corr(method = "spearman").abs()
#spearman_corr

# For categorical/nominal variables
# Simple label encoding just for correlation (not for model)
df_encoded = df_nan.copy()
for col in cat_cols:
    df_encoded[col] = df_nan[col].astype('category').cat.codes
#print(df_encoded)
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    return np.sqrt(phi2 / min(k - 1, r - 1))

cramers_results = pd.DataFrame(index=cat_cols, columns=cat_cols)

for col1 in cat_cols:
    for col2 in cat_cols:
        if col1 == col2:
            cramers_results.loc[col1, col2] = 1.0
        else:
            val = cramers_v(df_encoded[col1], df_encoded[col2])
            cramers_results.loc[col1, col2] = round(val, 3)
print("\nCramér's V (Nominal Features):")
print(cramers_results)

```

Plot the correlation matrix and find highly correlated columns
```{python}
fig, ax = plt.subplots(figsize=(20,20))
sns.heatmap(spearman_corr, annot = True, cmap = "coolwarm", ax= ax)
ax.set_title("Spearman Rank Correlation for Ordinal Variables")

fig, ax = plt.subplots(figsize=(30,30))
sns.heatmap(cramers_results.astype(float), ax = ax, annot = True, cmap = "coolwarm")
ax.set_title("Cramer's V correlation for categorical variables")
```

```{python}
def find_high_correlations(corr_matrix, threshold=0.8):
    correlated_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                col1 = corr_matrix.columns[i]
                col2 = corr_matrix.columns[j]
                correlated_pairs.append((col1, col2, corr_matrix.iloc[i, j]))
    return correlated_pairs

correlated_ord_cols = find_high_correlations(spearman_corr.astype(float), threshold = 0.7)
print("\nHighly correlated ordinal features (spearman ran correlation > 0.7):")
print(correlated_ord_cols)

correlated_cat_cols = find_high_correlations(cramers_results.astype(float), threshold = 0.7)
print("\nHighly correlated nominal features (Cramér's V > 0.7):")
print(correlated_cat_cols)
```

Collect correlated columns to drop

```{python}
corr_ord_cols_to_drop = set()
for col1, col2, corr in correlated_ord_cols:
    corr_ord_cols_to_drop.add(col2)
#corr_ord_cols_to_drop = list(set(corr_ord_cols_to_drop))
corr_ord_cols_to_drop = list(corr_ord_cols_to_drop)

corr_cat_cols_to_drop = set()
for col1, col2, corr in correlated_cat_cols:
    corr_cat_cols_to_drop.add(col2)
corr_cat_cols_to_drop = list(corr_cat_cols_to_drop)

corr_cols_to_drop = corr_ord_cols_to_drop+corr_cat_cols_to_drop

ord_cols_mod = [col for col in ord_cols if col not in corr_ord_cols_to_drop]
cat_cols_mod = [col for col in cat_cols if col not in corr_cat_cols_to_drop]
print(ord_cols_mod, cat_cols_mod)
```

Double-check the correlations to make sure all correlations are below 0.8 now

```{python}
df_nan = df_nan.drop(columns = corr_cols_to_drop)
print(df_nan.shape[1])
spearman_corr = df_nan[ord_cols_mod].corr(method = "spearman").abs()

df_encoded = df_nan.copy()
for col in cat_cols_mod:
    df_encoded[col] = df_nan[col].astype('category').cat.codes
cramers_results = pd.DataFrame(index=cat_cols_mod, columns=cat_cols_mod)

for col1 in cat_cols_mod:
    for col2 in cat_cols_mod:
        if col1 == col2:
            cramers_results.loc[col1, col2] = 1.0
        else:
            val = cramers_v(df_encoded[col1], df_encoded[col2])
            cramers_results.loc[col1, col2] = round(val, 3)
#print("\nCramér's V (Nominal Features):")
#print(cramers_results)
#spearman_corr

print("\nHighly correlated ordinal features (spearman ran correlation > 0.7):")
print(find_high_correlations(spearman_corr.astype(float), threshold = 0.7))
 
print("\nHighly correlated nominal features (Cramér's V > 0.7):")
print(find_high_correlations(cramers_results.astype(float), threshold = 0.7))

fig, ax = plt.subplots(figsize=(20,20))
sns.heatmap(spearman_corr, annot = True, cmap = "coolwarm", ax= ax)
ax.set_title("Spearman Rank Correlation for Ordinal Variables")

fig, ax = plt.subplots(figsize=(30,30))
sns.heatmap(cramers_results.astype(float), ax = ax, annot = True, cmap = "coolwarm")
ax.set_title("Cramer's V correlation for categorical variables")

```


Drop the highly correlated ordinal and categorical columns from X_train and X_test

```{python}
X_train = X_train.drop(columns = corr_cols_to_drop, inplace = False)
X_test = X_test.drop(columns = corr_cols_to_drop, inplace = False)
```

Ideally I would like to tune the threshold for getting the best model. But for now, I am fixing the threshold at 0.7

## 1. Clean the data - fix or remove outliers, fill in mssing values or drop their rows or columns.

## 2. Perform feature selection (optional). Drop the attributes that provide no useful information for the task - done

## 3. Perform feature engineering, where appropriate.
    - Discretize continuous features
    - Decompose features e.g. ccategorical, date/time etc.
    - Add promising transformations of features. E.g log(x), sqrt(x), x2 etc.
    - Aggregate features into promising new features.

## 4. perform feature scaling
    - Standardize or normalize features

```{python}
print(f"Number of features in the data: {X_train.shape[1]}")
```

Define the pipeline steps for ordinal and categorical columns
```{python}
ord_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = check_special_missing, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy='most_frequent', add_indicator = False)),
    ("min_max_scaler", MinMaxScaler())
    ])
cat_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = check_special_missing, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy = "most_frequent", add_indicator = False)),
    ("one_hot_encoder", OneHotEncoder(sparse_output=False, handle_unknown="ignore", drop = "first"))
])
```

# Shortlist Promising models

- I am going to look at LogisticRegression, RandomForest and LightGBM

## Try LogisticRegression

```{python}
preprocessing = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols_mod),
    ("ord", ord_pipeline, ord_cols_mod)
])

lr = LogisticRegression(class_weight='balanced',random_state=42)
lr_clf = Pipeline([
("pre_processing", preprocessing),
("logistic_regression", lr)
])
lr_clf.fit(X_train, y_train)

lr_cv = cross_val_score(lr_clf, X_train, y_train, cv = 5, scoring = "f1")
lr_cv
```

Feature importance for logistic regression model
```{python}
coefficients = np.abs(lr_clf[1].coef_[0])
#coefficients
#len(coefficients)


# # Display feature importance using coefficients and odds ratios
feature_importance_lr = pd.DataFrame({
    'Feature': lr_clf[0].get_feature_names_out(),
    'Coefficient': coefficients
}).sort_values(by='Coefficient', ascending=False)
print("\nFeature Importance in Logistic Regression (Absolute value of coefficients):")
print(feature_importance_lr)
```

## Try LightGBM

```{python}
from lightgbm import LGBMClassifier
lgbm = LGBMClassifier(class_weight='balanced', random_state=42)

lgbm_clf = Pipeline([
("pre_processing", preprocessing),
("light_gbm", lgbm)
])
lgbm_clf.fit(X_train, y_train)

lgbm_cv = cross_val_score(lgbm_clf, X_train, y_train, cv = 5, scoring = "f1")
lgbm_cv
```

Feature importance for lightGBM model
```{python}
importances = lgbm_clf[1].feature_importances_
feature_names = lgbm_clf[0].get_feature_names_out()
# # Display feature importance using coefficients and odds ratios
feature_importance_light_gbm = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)
print("\nFeature Importance in lightGBM:")
print(feature_importance_light_gbm)
```

## Try RandomForest
```{python}
rf = RandomForestClassifier(class_weight='balanced', random_state=42)
forest_clf = Pipeline([
("pre_processing", preprocessing),
("random_forest", rf)
])
forest_clf.fit(X_train, y_train)
forest_cv = cross_val_score(forest_clf, X_train, y_train, cv = 5, scoring = "f1")
forest_cv
# From previous finetuning, I know that the performance of random forest improves a lot with finetuning
```

Random Forest model with custom parameters
```{python}
rf = RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=300, min_samples_split=2, min_samples_leaf=4, max_depth = 40)
forest_clf = Pipeline([
("pre_processing", preprocessing),
("random_forest", rf)
])
forest_clf.fit(X_train, y_train)
forest_cv = cross_val_score(forest_clf, X_train, y_train, cv = 5, scoring = "f1")
forest_cv
```

Feature importance for randomForest model
```{python}
importances = forest_clf[1].feature_importances_
feature_names = forest_clf[0].get_feature_names_out()
# # Display feature importance using coefficients and odds ratios
feature_importance_forest = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)
print("\nFeature Importance in RandomForest:")
print(feature_importance_forest)
```

So far, the random forest model has the best f1 score. logistic regression and lightGBM has comparable scores.
- I will also need to check if any of the models are overfitting or underfitting.

## Look at the overlap between top 20 features from each model

```{python}

# Get top 20 features from each importance column
top20_lr = set(feature_importance_lr.nlargest(20, 'Coefficient')['Feature'])
top20_light_gbm = set(feature_importance_light_gbm.nlargest(20, 'Importance')['Feature'])
top20_forest = set(feature_importance_forest.nlargest(20, 'Importance')['Feature'])

# Find common features across all three top 20 sets
common_all = top20_lr & top20_light_gbm & top20_forest

# Output
print("Common in all three:", common_all, len(common_all))

```


## Calculate probability scores and predictions for all three models
```{python}
y_pred_lr = cross_val_predict(lr_clf, X_train, y_train, cv = 5)
y_pred_forest = cross_val_predict(forest_clf, X_train, y_train, cv = 5)
y_pred_lgbm= cross_val_predict(lgbm_clf, X_train, y_train, cv = 5)
y_scores_lr = cross_val_predict(lr_clf, X_train, y_train, cv = 5, method = "predict_proba")
y_scores_forest = cross_val_predict(forest_clf, X_train, y_train, cv = 5, method = "predict_proba")
y_scores_lgbm= cross_val_predict(lgbm_clf, X_train, y_train, cv = 5, method = "predict_proba")
```


## Let's look at the precision recall curves for all three models
```{python}
# Get cross-validated predicted probabilities (for the positive class)

fig, ax = plt.subplots(figsize=(8, 6))
def plot_precision_recall_curve(title, y, y_scores):

    # Compute precision-recall curve
    precision, recall, thresholds = precision_recall_curve(y, y_scores)
    avg_precision = average_precision_score(y, y_scores)

    # Plotting
    ax.plot(recall, precision, label=f'{title} (AP = {avg_precision:.2f})')
    ax.set_xlabel('Recall')
    ax.set_ylabel('Precision')
    ax.set_title(f'Precision-Recall Curve (Cross-Validated) for {title}')
    ax.legend()
    ax.grid(True)
    #plt.show()
    #plt.close()

for title, y_scores in zip(["LogisticRegression", "LightGBM", "RandomForest"],[y_scores_lr[:, 1],y_scores_forest[:, 1], y_scores_lgbm[:, 1]]):
    plot_precision_recall_curve(title, y_train, y_scores)
```

# Error analysis for each type of model

## Error analysis for linear regression

### Look at the learning curves
```{python}
from sklearn.model_selection import learning_curve

train_sizes, train_scores, valid_scores = learning_curve(
    lr_clf, X_train, y_train, train_sizes=np.linspace(0.01, 1.0, 10), cv=5,
    scoring="f1")
train_errors = -train_scores.mean(axis=1)
valid_errors = -valid_scores.mean(axis=1)

plt.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train")
plt.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")
#[...]  # beautify the figure: add labels, axis, grid, and legend
plt.legend()
plt.show()
```

### Confusion Matrices
```{python}
ConfusionMatrixDisplay.from_predictions(y_train, y_pred_lr)
ConfusionMatrixDisplay.from_predictions(y_train, y_pred_lr, normalize = "true", values_format="0.2%")

ConfusionMatrixDisplay.from_predictions(y_train, y_pred_forest, normalize = "pred", values_format="0.2%")
```

Change the prediciton threshold
```{python}
threshold = 0.3
y_pred_lr_custom = (y_scores_lr[:,1]> threshold)
ConfusionMatrixDisplay.from_predictions(y_train, y_pred_lr_custom, normalize = "true", values_format="0.2%")
```


### Visualize probabilities for false postives and false negatives 
```{python}
# 2. Identify misclassifications
false_positives = (y_pred_lr == 1) & (y_train == 0)
false_negatives = (y_pred_lr == 0) & (y_train == 1)
correct = (y_pred_lr == y_train)

print(f"False Positives: {np.sum(false_positives)}")
print(f"False Negatives: {np.sum(false_negatives)}")

# ----------------------------------
# A. Histogram of FP and FN
# ----------------------------------
plt.figure(figsize=(10, 5))
plt.hist(y_scores_lr[:,1][false_positives], bins=20, alpha=0.6, label='False Positives')
plt.hist(y_scores_lr[:,1][false_negatives], bins=20, alpha=0.6, label='False Negatives')
plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.title('Histogram of Misclassified Probabilities')
plt.legend()
plt.show()
#
```

### Error breakdown
```{python}

results = pd.DataFrame({
    'actual': y_train,
    'predicted': y_pred_lr,
    'proba': y_scores_lr[:, 1]
})
results['error_type'] = results.apply(
    lambda row: 'TP' if row.actual == 1 and row.predicted == 1 else
                'TN' if row.actual == 0 and row.predicted == 0 else
                'FP' if row.actual == 0 and row.predicted == 1 else
                'FN', axis=1
)
results_lr = results.copy()
# Should look at the features for which FP and FNs are high
```


### Look at SHAP values for false positives and false negatives
```{python}
import shap
# ----------------------------------
# C. SHAP Explanations
# ----------------------------------
print("Generating SHAP explanations...")
X_transformed = lr_clf.named_steps['pre_processing'].transform(X_train)
model = lr_clf.named_steps["logistic_regression"]
feature_names = lr_clf[0].get_feature_names_out()

explainer = shap.Explainer(model, X_transformed)
shap_values_raw = explainer(X_transformed)

# Step 4: Wrap each SHAP value into Explanation with feature names
shap_values = shap.Explanation(
    values=shap_values_raw.values,
    base_values=shap_values_raw.base_values,
    data=shap_values_raw.data,
    feature_names=feature_names
)

# Show SHAP for 1 FP and 1 FN
if np.any(false_positives):
    idx_fp = np.where(false_positives)[0][0]
    print("\nSHAP Explanation for a False Positive:")
    shap.plots.waterfall(shap_values[idx_fp])

if np.any(false_negatives):
    idx_fn = np.where(false_negatives)[0][0]
    print("\nSHAP Explanation for a False Negative:")
    shap.plots.waterfall(shap_values[idx_fn])

# Use in SHAP summary plot
shap.summary_plot(shap_values, features=X_transformed)
```

## Error analysis for random forest

### Learning curve
```{python}

train_sizes, train_scores, valid_scores = learning_curve(
    forest_clf, X_train, y_train, train_sizes=np.linspace(0.01, 1.0, 10), cv=5,
    scoring="f1")
train_errors = -train_scores.mean(axis=1)
valid_errors = -valid_scores.mean(axis=1)

plt.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train")
plt.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")
#[...]  # beautify the figure: add labels, axis, grid, and legend
plt.legend()
plt.title("Learning curves for RandomForest model")
plt.show()
```

The learning curve shows that the RnadomForest Model is overfitting the data. There is a big gap between the training and validation curve

### Confusion matrices for random forests
```{python}
ConfusionMatrixDisplay.from_predictions(y_train, y_pred_forest, normalize = "true", values_format="0.2%")

ConfusionMatrixDisplay.from_predictions(y_train, y_pred_forest, normalize = "pred", values_format="0.2%")

threshold = 0.2
y_pred_forest_custom = (y_scores_forest[:,1]> threshold)
ConfusionMatrixDisplay.from_predictions(y_train, y_pred_forest_custom, normalize = "true", values_format="0.2%")

```


### Visualizing probability distributions for RandomForest
```{python}
# 2. Identify misclassifications
false_positives = (y_pred_forest == 1) & (y_train == 0)
false_negatives = (y_pred_forest == 0) & (y_train == 1)
correct = (y_pred_forest == y_train)

print(f"False Positives: {np.sum(false_positives)}")
print(f"False Negatives: {np.sum(false_negatives)}")

# ----------------------------------
# A. Histogram of FP and FN
# ----------------------------------
plt.figure(figsize=(10, 5))
plt.hist(y_scores_forest[:,1][false_positives], bins=20, alpha=0.6, label='False Positives')
plt.hist(y_scores_forest[:,1][false_negatives], bins=20, alpha=0.6, label='False Negatives')

plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.title('Histogram of Misclassified Probabilities')
plt.legend()
plt.show()
```

For random forest, visualizing SHAP values is too slow.

## Error analysis for lightGBM

### Learning Curve
```{python}

train_sizes, train_scores, valid_scores = learning_curve(
    lgbm_clf, X_train, y_train, train_sizes=np.linspace(0.01, 1.0, 10), cv=5,
    scoring="f1")
train_errors = -train_scores.mean(axis=1)
valid_errors = -valid_scores.mean(axis=1)

plt.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train")
plt.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")
#[...]  # beautify the figure: add labels, axis, grid, and legend
plt.legend()
plt.title("Learning curves for lightGBM model")
plt.show()
```

LightGBM is also overfitting the training data because there is a difference between the training and validation curves.

### Confusion matrices for light GBM
```{python}
ConfusionMatrixDisplay.from_predictions(y_train, y_pred_lgbm, normalize = "true", values_format="0.2%")

ConfusionMatrixDisplay.from_predictions(y_train, y_pred_lgbm, normalize = "pred", values_format="0.2%")

threshold = 0.2
y_pred_lgbm_custom = (y_scores_lgbm[:,1]> threshold)
ConfusionMatrixDisplay.from_predictions(y_train, y_pred_lgbm_custom, normalize = "true", values_format="0.2%")

```


### Visualizing probability distributions for RandomForest
```{python}
# 2. Identify misclassifications
false_positives = (y_pred_lgbm == 1) & (y_train == 0)
false_negatives = (y_pred_lgbm == 0) & (y_train == 1)
correct = (y_pred_lgbm == y_train)

print(f"False Positives: {np.sum(false_positives)}")
print(f"False Negatives: {np.sum(false_negatives)}")

# ----------------------------------
# A. Histogram of FP and FN
# ----------------------------------
plt.figure(figsize=(10, 5))
plt.hist(y_scores_lgbm[:,1][false_positives], bins=20, alpha=0.6, label='False Positives')
plt.hist(y_scores_lgbm[:,1][false_negatives], bins=20, alpha=0.6, label='False Negatives')
plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.title('Histogram of Misclassified Probabilities')
plt.legend()
plt.show()
```

So far, Logistic Regression is underfitting, tuned RandomForest is massively overfitting and lightGBM is also underfitting. The validation curve is plateauing at about 5000 training datasets and the training curve continues to get worse as we increase data.

What are my next options?
Try feature selection?

Work with logistic regression and lightGBM going forward for now.

# Use age as categorical variable rather than ordinal
The first thing would be to test if age can be represented as a categorical variable and see if it makes a difference.
```{python}
ord_cols_mod, cat_cols_mod
# Move age from ordinal columns to categorical columns
ord_cols_mod = [col for col in ord_cols_mod if col != "DHHGAGE"]
cat_cols_mod = list(set(cat_cols_mod + ["DHHGAGE"]))
ord_cols_mod, cat_cols_mod
```

## Create logisitc regression pipeline again

```{python}
preprocessing = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols_mod),
    ("ord", ord_pipeline, ord_cols_mod)
])

lr = LogisticRegression(class_weight='balanced',random_state=42, max_iter = 500)
lr_clf = Pipeline([
("pre_processing", preprocessing),
("logistic_regression", lr)
])
lr_clf.fit(X_train, y_train)

lr_cv = cross_val_score(lr_clf, X_train, y_train, cv = 5, scoring = "f1")
lr_cv
```

Feature importance
```{python}
coefficients = np.abs(lr_clf[1].coef_[0])
#coefficients
#len(coefficients)


# # Display feature importance using coefficients and odds ratios
feature_importance_lr = pd.DataFrame({
    'Feature': lr_clf[0].get_feature_names_out(),
    'Coefficient': coefficients
}).sort_values(by='Coefficient', ascending=False)
print("\nFeature Importance in Logistic Regression (Absolute value of coefficients):")
print(feature_importance_lr)
```

Learning curve
```{python}
train_sizes, train_scores, valid_scores = learning_curve(
    lr_clf, X_train, y_train, train_sizes=np.linspace(0.01, 1.0, 10), cv=5,
    scoring="f1")
train_errors = -train_scores.mean(axis=1)
valid_errors = -valid_scores.mean(axis=1)

plt.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train")
plt.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")
#[...]  # beautify the figure: add labels, axis, grid, and legend
plt.legend()
plt.title("Learning curves for Logistic Regression classifier")
plt.xlabel("Training data size")
plt.ylabel("-f1 score")
plt.show()


```

Classification report
```{python}
y_pred_lr = cross_val_predict(lr_clf, X_train, y_train, cv = 5)
y_scores_lr = cross_val_predict(lr_clf, X_train, y_train, cv = 5, method = "predict_proba")
```

```{python}
print("Logistic Regression classification report")
Markdown(classification_report(y_train, y_pred_lr))
```


# Build a model with a few manually selected variables
Top 10 variables from logistic regression model are:
1. DHHGAGE - age
2. GEN_005 - perceived health
3. CCC_080 - took medication for high blood cholesterol
4. ALC_015 - frequency of drinking alcohol
5. CCC_070 - took medication for high blood pressure
6. SDCDVFLA - visible minority
7. HWTDGBCC - BMI classification
8. DHH_SEX - Sex at Birth
9. SMK_005 - type of smoker - daily, occasionally, not at all
10. GEN_015 - Perceived mental health

Selected variables:
1. DHHGAGE
3. CCC_080 
5. CCC_070

```{python}
manually_selected_cols = ["DHHGAGE", "CCC_080", "CCC_070"]
X_test = X_test[manually_selected_cols]
X_train = X_train[manually_selected_cols]

for col in manually_selected_cols:
    print(col)
    print(X_train[col].value_counts()/(X_train.shape[0]), X_test[col].value_counts()/(X_test.shape[0]))

df_train_copy = df_train_set[manually_selected_cols+["CCC_095"]]
df_train_nan = check_special_missing(df_train_copy)
```

## Univariate Analysis
```{python}
for col in df_train_nan.columns:
    sns.countplot(x = col, data = df_train_nan)
    plt.show()
    plt.close()

```

## Bivariate analysis

```{python}
# Boxplots
for col in manually_selected_cols:
    sns.countplot(x = col, hue = "CCC_095", data = df_train_nan)
    plt.show()
    plt.close()
```

## Define ordinal and categorical columns 
```{python}
ord_cols = []
cat_cols = [col for col in manually_selected_cols if col not in ord_cols]
```

```{python}
ord_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = check_special_missing, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy='most_frequent', add_indicator = False)),
    ("min_max_scaler", MinMaxScaler())
    ])
cat_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = check_special_missing, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy = "most_frequent", add_indicator = False)),
    ("one_hot_encoder", OneHotEncoder(sparse_output=False, handle_unknown="ignore", drop = "first"))
])
```

Define Logistc regression model
```{python}
preprocessing = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols),
    ("ord", ord_pipeline, ord_cols)
])

lr = LogisticRegression(class_weight='balanced',random_state=42)
lr_clf = Pipeline([
("pre_processing", preprocessing),
("logistic_regression", lr)
])
lr_clf.fit(X_train, y_train)

lr_cv = cross_val_score(lr_clf, X_train, y_train, cv = 5, scoring = "f1")
lr_cv
```

```{python}
y_pred_lr_manual = cross_val_predict(lr_clf, X_train, y_train, cv = 5)
```

```{python}
print("Logistic Regression classification report for manual columns")
Markdown(classification_report(y_train, y_pred_lr_manual))
```

## Error analysis - let's look at false positives and false negatives in the data
```{python}

results = pd.DataFrame({
    'actual': y_train,
    'predicted': y_pred_lr_manual
})
results['error_type'] = results.apply(
    lambda row: 'TP' if row.actual == 1 and row.predicted == 1 else
                'TN' if row.actual == 0 and row.predicted == 0 else
                'FP' if row.actual == 0 and row.predicted == 1 else
                'FN', axis=1
)
#results_lr_manual = results.copy()
results_lr_manual = pd.concat([X_train, results], axis = 1)
results_lr_manual_nan = check_special_missing(results_lr_manual)
```


### Let's look at false positives and false negatives
```{python}
fp_df = results_lr_manual[results_lr_manual["error_type"] == "FP"]
fn_df = results_lr_manual[results_lr_manual["error_type"] == "FN"]

fp_df_nan = check_special_missing(fp_df)
fn_df_nan = check_special_missing(fn_df)
```

False positives
```{python}
for col in manually_selected_cols:
    fig, ax = plt.subplots(1, 2)
    sns.countplot(x = col, hue = "predicted",data = fp_df_nan, ax = ax[0])
    sns.countplot(x = col, data = df_train_nan, ax = ax[1])
    plt.show(fig)
    plt.close(fig)
```
False negatives

```{python}
for col in manually_selected_cols:
    fig, ax = plt.subplots(1, 2)
    sns.countplot(x = col, data = fn_df_nan, ax = ax[0])
    sns.countplot(x = col, data = df_train_nan, ax = ax[1])
    plt.show(fig)
    plt.close(fig)
```

```{python}
print("Looking at error distribution for DHHGAGE")
col = "DHHGAGE"
for value in results_lr_manual[col].unique():
    print(value)
    df = results_lr_manual_nan[results_lr_manual_nan[col] == value]
    print(df["error_type"].value_counts()/df.shape[0])
col = "CCC_080"
print("Looking at error distribution for CCC_080")
for value in results_lr_manual[col].unique():
    print(value)
    df = results_lr_manual_nan[results_lr_manual_nan[col] == value]
    print(df["error_type"].value_counts()/df.shape[0])

print("Looking at error distribution for CCC_070")    
col = "CCC_070"
for value in results_lr_manual[col].unique():
    print(value)
    df = results_lr_manual_nan[results_lr_manual_nan[col] == value]
    print(df["error_type"].value_counts()/df.shape[0])
```

The false positives and false negatives are certainly not random and there are more false positives in certain categories that others. With false negatives, as well, there is a pattern.


## Try a model after shuffling labels
```{python}
import random
y_train_shuffled = np.array(y_train.copy())
random.shuffle(y_train_shuffled)

lr_cv_shuffled = cross_val_score(lr_clf, X_train, y_train_shuffled, cv = 5, scoring = "f1")
lr_cv_shuffled
```

```{python}
y_pred_lr_manual_shuffled = cross_val_predict(lr_clf, X_train, y_train_shuffled, cv = 5)
```

```{python}
print("Logistic Regression classification report for manual columns")
Markdown(classification_report(y_train, y_pred_lr_manual_shuffled))
```


## Now what if I undersample and make both classes equal

```{python}
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as imbPipeline
from collections import Counter

rus = RandomUnderSampler(random_state=0)
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)
print(sorted(Counter(y_resampled).items()))

#sm= SMOTE(sampling_strategy=0.3)
#print('Original dataset shape %s' % Counter(y_train))
#X_res, y_res = sm.fit_resample(X_train, y_train)
#print('Resampled dataset shape %s' % Counter(y_res))
#under = RandomUnderSampler(sampling_strategy=0.5)

preprocessing = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols),
    ("ord", ord_pipeline, ord_cols)
])

lr = LogisticRegression(random_state=42)
lr_clf_us = imbPipeline([
("pre_processing", preprocessing),
("under_sampling",rus),
("logistic_regression", lr)
])
lr_clf_us.fit(X_train, y_train)

lr_cv_us = cross_val_score(lr_clf_us, X_train, y_train, cv = 5, scoring = "f1")
lr_cv_us
```


```{python}
y_pred_lr_us = cross_val_predict(lr_clf_us, X_train, y_train, cv = 5)
```

```{python}
print("Logistic Regression classification report for manual columns")
Markdown(classification_report(y_train, y_pred_lr_us))
```

## Error analysis - let's look at false positives and false negatives in the data
```{python}

results = pd.DataFrame({
    'actual': y_train,
    'predicted': y_pred_lr_us
})
results['error_type'] = results.apply(
    lambda row: 'TP' if row.actual == 1 and row.predicted == 1 else
                'TN' if row.actual == 0 and row.predicted == 0 else
                'FP' if row.actual == 0 and row.predicted == 1 else
                'FN', axis=1
)
#results_lr_manual = results.copy()
results_lr_manual_us = pd.concat([X_train, results], axis = 1)
results_lr_manual_nan_us = check_special_missing(results_lr_manual_us)
```


```{python}
print("Looking at error distribution for DHHGAGE")
col = "DHHGAGE"
for value in results_lr_manual_us[col].unique():
    print(value)
    df = results_lr_manual_nan_us[results_lr_manual_nan_us[col] == value]
    print(df["error_type"].value_counts()/df.shape[0])
col = "CCC_080"
print("Looking at error distribution for CCC_080")
for value in results_lr_manual_us[col].unique():
    print(value)
    df = results_lr_manual_nan_us[results_lr_manual_nan_us[col] == value]
    print(df["error_type"].value_counts()/df.shape[0])

print("Looking at error distribution for CCC_070")    
col = "CCC_070"
for value in results_lr_manual_us[col].unique():
    print(value)
    df = results_lr_manual_nan_us[results_lr_manual_nan_us[col] == value]
    print(df["error_type"].value_counts()/df.shape[0])
```
```{python}
tp_df = results_lr_manual_nan_us[results_lr_manual_nan_us["error_type"] == "TP"]
print(tp_df.value_counts())

tn_df = results_lr_manual_nan_us[results_lr_manual_nan_us["error_type"] == "TN"]
print(tn_df.value_counts())

fp_df = results_lr_manual_nan_us[results_lr_manual_nan_us["error_type"] == "FP"]
print(fp_df.value_counts())

fn_df = results_lr_manual_nan_us[results_lr_manual_nan_us["error_type"] == "FN"]
print(fn_df.value_counts())

original_df = pd.concat([X_resampled, y_resampled], axis = 1)
pos_df = original_df[original_df["CCC_095"]== True]
print(pos_df.value_counts())

neg_df = original_df[original_df["CCC_095"]== False]
print(neg_df.value_counts())

```

## Use lgbm on undersampled data
```{python}
preprocessing = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols),
    ("ord", ord_pipeline, ord_cols)
])

lgbm = LGBMClassifier(random_state=42)
lgbm_clf_us = imbPipeline([
("pre_processing", preprocessing),
("under_sampling",rus),
("light_gbm", lgbm)
])
lgbm_clf_us.fit(X_train, y_train)

lgbm_cv_us = cross_val_score(lgbm_clf_us, X_train, y_train, cv = 5, scoring = "f1")
lgbm_cv_us

```


```{python}
y_pred_lgbm_us = cross_val_predict(lgbm_clf_us, X_train, y_train, cv = 5)
```

```{python}
print("Logistic Regression classification report for manual columns")
Markdown(classification_report(y_train, y_pred_lgbm_us))
```

## Error analysis - let's look at false positives and false negatives in the data
```{python}

results = pd.DataFrame({
    'actual': y_train,
    'predicted': y_pred_lgbm_us
})
results['error_type'] = results.apply(
    lambda row: 'TP' if row.actual == 1 and row.predicted == 1 else
                'TN' if row.actual == 0 and row.predicted == 0 else
                'FP' if row.actual == 0 and row.predicted == 1 else
                'FN', axis=1
)
#results_lr_manual = results.copy()
results_lgbm_manual_us = pd.concat([X_train, results], axis = 1)
results_lgbm_manual_nan_us = check_special_missing(results_lgbm_manual_us)
```

```{python}
tp_df = results_lgbm_manual_nan_us[results_lgbm_manual_nan_us["error_type"] == "TP"]
print(tp_df.value_counts())

tn_df = results_lgbm_manual_nan_us[results_lgbm_manual_nan_us["error_type"] == "TN"]
print(tn_df.value_counts())

fp_df = results_lgbm_manual_nan_us[results_lgbm_manual_nan_us["error_type"] == "FP"]
print(fp_df.value_counts())

fn_df = results_lgbm_manual_nan_us[results_lgbm_manual_nan_us["error_type"] == "FN"]
print(fn_df.value_counts())

```

Even though the classification report for both lightGBM and logistic regression looks the same, lightGBM performs better on some obvious samples.

# Perform feature selection

## Recursive feature selection
```{python}
from sklearn.feature_selection import RFECV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold
X_transformed = lr_clf[0].fit_transform(X_train)

min_features_to_select = 1
cv = StratifiedKFold(5)
rfecv = RFECV(
    estimator=lr_clf[1],
    step=1,
    cv=cv,
    scoring="f1",
    min_features_to_select=min_features_to_select,
    n_jobs=2,
)
rfecv.fit(X_transformed, y_train)

print(f"Optimal number of features: {rfecv.n_features_}")
```



```{python}
cv_results = pd.DataFrame(rfecv.cv_results_)
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Mean test accuracy")
plt.errorbar(
    x=cv_results["n_features"],
    y=cv_results["mean_test_score"],
    yerr=cv_results["std_test_score"],
)
plt.title("Recursive Feature Elimination \nwith correlated features")
plt.show()
cv_results
```

The results show that top 10 features are more than enough for logistic regression atleast

## Univariate feature selection with logistic regression

```{python}
from sklearn.feature_selection import SelectKBest, mutual_info_classif

mic = mutual_info_classif(X_transformed, y_train, discrete_features=True, random_state = 42)
mic_pd = pd.DataFrame({"Features" : lr_clf[0].get_feature_names_out(), "mic_score" : mic}).sort_values(by = "mic_score", ascending=False)
mic_pd
```

```{python}
selector.pvalues_
# X_indices = np.arange(X_transformed.shape[-1])
# plt.figure(1)
# plt.clf()
# plt.bar(X_indices - 0.05, scores, width=0.2)
# plt.title("Feature univariate score")
# plt.xlabel("Feature number")
# plt.ylabel(r"Univariate score ($-Log(p_{value})$)")
# plt.show()
```

# let's make a pipeline to reduce dimensions
```{python}
preprocessing = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols_mod),
    ("ord", ord_pipeline, ord_cols_mod)
])

lr = LogisticRegression(class_weight='balanced',random_state=42, max_iter = 500)
lr_clf_reduced = Pipeline([
("pre_processing", preprocessing),
("reduce_dim", "passthrough"),
("logistic_regression", lr)
])

N_FEATURES_OPTIONS = [2, 4, 8, 10, 20]
param_grid = [{
    "reduce_dim": [SelectKBest(mutual_info_classif)],
    "reduce_dim__k": N_FEATURES_OPTIONS
}]

grid = GridSearchCV(lr_clf_reduced, n_jobs=1, param_grid=param_grid, scoring = "f1")
grid.fit(X_train, y_train)
```

```{python}
mean_scores = np.array(grid.cv_results_["mean_test_score"])
mean_scores

pd.DataFrame(grid.cv_results_).sort_values(by = "mean_test_score", ascending = False)
# # scores are in the order of param_grid iteration, which is alphabetical
# mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))
# # select score for best C
# mean_scores = mean_scores.max(axis=0)
# # create a dataframe to ease plotting
# mean_scores = pd.DataFrame(
#     mean_scores.T, index=N_FEATURES_OPTIONS, columns=reducer_labels
# )

# ax = mean_scores.plot.bar()
# ax.set_title("Comparing feature reduction techniques")
# ax.set_xlabel("Reduced number of features")
# ax.set_ylabel("Digit classification accuracy")
# ax.set_ylim((0, 1))
# ax.legend(loc="upper left")

# plt.show()
```

```{python}
N_FEATURES_OPTIONS = [20, 30, 40, 50, 56]
param_grid = [{
    "reduce_dim": [SelectKBest(mutual_info_classif)],
    "reduce_dim__k": N_FEATURES_OPTIONS
}]

grid_extend = GridSearchCV(lr_clf_reduced, n_jobs=1, param_grid=param_grid, scoring = "f1")
grid_extend.fit(X_train, y_train)
```

```{python}
pd.DataFrame(grid_extend.cv_results_).sort_values(by = "mean_test_score", ascending = False)
```



# TO DO
- Discussion with Raaisa - 2025_05_20
- Try a baseline model by shuffling diabetes labels - Tried, the f1 score is around 0.19
- Build a model with only three features and take it to the best f1 score - the best performance is an f1 score of 0.36. lightGBM is better than logistic regression, which makes some obvious errors. With 3 variables, it is obvious that the model cannot do any better as the variables are all discrete. So there are a limited number of categories and there is an upper limit to what the model can predict.

- Look at the false predictions and see if there is a trend - age, cholesterol, blood pressure
- Look at stratification by other variables
- Look at diabetes prediction model on kaggle - https://www.kaggle.com/code/tumpanjawat/diabetes-eda-random-forest-hp
- Run with balanced dataset by sampling from non-diabetes people - I undersampled the majority class because we have 10000 samples for minority class and it did not make much of a difference.
