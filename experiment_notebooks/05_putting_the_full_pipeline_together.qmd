---
title: "End-to-end pipeline for prediction of diabetes status from Canadian Community Health Survey Data"
author: "Shefali Lathwal"
date: "2025-05-21"
date-modified: last-modified
toc: true
format: html
jupyter: python3
echo: true
---

# Go to root directory of the project
```{python}
#import os
print(os.getcwd())
os.chdir(os.path.abspath(".."))
print(os.getcwd())
```

# Import required libraries
```{python}
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from src.data_preparation import keep_known_codes_in_column, remove_known_codes_in_column
from src.data_preparation import convert_missing_codes_to_na
from src.data_preparation import separate_df_into_train_and_test
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate, StratifiedKFold, StratifiedShuffleSplit, RandomizedSearchCV
from sklearn.utils import shuffle
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from src.data_preparation import return_columns_with_missing_data, cols_with_zero_variance, additional_cols_to_exclude, cols_with_high_correlation
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline, FunctionTransformer
from src.data_plotting import plot_precision_recall_curve
from sklearn.model_selection import learning_curve
from sklearn.metrics import precision_score, recall_score, classification_report
from IPython.display import Markdown
from scipy.stats import randint, uniform
from sklearn.svm import SVC
from xgboost import XGBClassifier
```

# Import the data and keep only the relevant rows
Keep data with only known diabetes status and adults (age 18+)
```{python}
df = pd.read_csv("/Users/shefalilathwal/Documents/diabetes_canada_project/data/pumf_cchs.csv")

target_col = "CCC_095" # Column with diabetes status
# Only keeping rows with a known diabetes status - 1 or 2
df = keep_known_codes_in_column(df, target_col, [1.0, 2.0])

# Removing rows for youth (< 18 years of age), code = 1
df = remove_known_codes_in_column(df, "DHHGAGE", [1.0])
print(df.shape)
```

# Split the data into training and test sets
```{python}
df_train_set, df_test_set = train_test_split(df, test_size = 0.3, random_state = 42, shuffle = True, stratify = df[target_col])

X_train, y_train = separate_df_into_train_and_test(df_train_set, target_col)
X_test, y_test = separate_df_into_train_and_test(df_test_set, target_col)

# Convert the target variable into True and False
y_train, y_test = (y_train == 1), (y_test == 1)
print(f"Distribution of classes in training data: {sorted(Counter(y_train).items())}")
y_train.shape
```

# Collect features to drop from the dataset
Drop features with:
- more than 30% data missing
- with same value in all rows
- used as flag or as numbers
- with high correlation with other features
```{python}
cols_to_exclude = additional_cols_to_exclude(X_train)
cols_to_drop = cols_to_exclude
X_train = X_train.drop(columns = cols_to_drop, inplace = False)
X_test = X_test.drop(columns = cols_to_drop, inplace = False)
print(f"Number of features after dropping flag and id columns: {X_train.shape[1]}")
print(cols_to_drop)

cols_with_30_pct_missing_data = return_columns_with_missing_data(X_train, pct_threshold=30)

cols_to_drop = cols_with_30_pct_missing_data
X_train = X_train.drop(columns = cols_to_drop, inplace = False)
X_test = X_test.drop(columns = cols_to_drop, inplace = False)
print(f"Number of features after dropping columns with more than 30 percent data missing: {X_train.shape[1]}")
print(cols_to_drop)

zero_variance_cols = cols_with_zero_variance(X_train)

cols_to_drop = zero_variance_cols
X_train = X_train.drop(columns = cols_to_drop, inplace = False)
X_test = X_test.drop(columns = cols_to_drop, inplace = False)
print(f"Number of features after dropping zero variance columns: {X_train.shape[1]}")
print(cols_to_drop)


ord_cols = ["INCDGRRS", "INCDGRPR", "INCDGRCA", "INCDGHH", "FSCDVHF2","FSCDVAF2", "PNCDVNED", "PNCDVPNO", "PNCDVPNC", "PNCDVPNM", "PNCDVPNI", "PNCDVHCT", "PHC_035", "ALCDVTTM","ALC_020", "ALC_015", "SMKDVSTY", "SMK_005","GENDVSWL","GENDVMHI","GENDVHDI","GEN_030","GEN_020","GEN_015","GEN_010","GEN_005","EHG2DVH3","DHHGAGE"]
cat_cols = [col for col in X_train.columns if col not in ord_cols]

ord_cols_to_drop, cat_cols_to_drop = cols_with_high_correlation(X_train, ord_cols, cat_cols, corr_threshold=0.7)
corr_cols_to_drop = ord_cols_to_drop+cat_cols_to_drop
cols_to_drop = corr_cols_to_drop
X_train = X_train.drop(columns = cols_to_drop, inplace = False)
X_test = X_test.drop(columns = cols_to_drop, inplace = False)
print(f"Number of features after dropping correlated columns: {X_train.shape[1]}")

ord_cols_mod = [col for col in ord_cols if col not in ord_cols_to_drop]
cat_cols_mod = [col for col in cat_cols if col not in cat_cols_to_drop]
```

# Undersample the majority class to balance both classes in the training data
```{python}
rus = RandomUnderSampler(random_state=0)
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)
print(f"Number of features in the data: {X_train.shape[1]}")
print(f"Nubmer of rows in the training data: {X_train.shape[0]}")
print(f"Distribution of classes: {sorted(Counter(y_resampled).items())}")
```

# Define the pre-processing pipeline for ordinal and categorical variables
```{python}
ord_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = convert_missing_codes_to_na, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy='most_frequent', add_indicator = False)),
    ("min_max_scaler", MinMaxScaler())
    ])
cat_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = convert_missing_codes_to_na, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy = "most_frequent", add_indicator = False)),
    ("one_hot_encoder", OneHotEncoder(sparse_output=False, handle_unknown="ignore", drop = "first"))
])


preprocessing = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols_mod),
    ("ord", ord_pipeline, ord_cols_mod)
])

X_processed = pd.DataFrame(preprocessing.fit_transform(X_train, y_train), index = X_train.index, columns = preprocessing.get_feature_names_out())
X_processed
```

# Fit a logistic regression model
```{python}
lr = LogisticRegression(random_state=42, penalty = None)
lr_clf = Pipeline([
("pre_processing", preprocessing),
("logistic_regression", lr)
])

lr_cv = cross_validate(lr_clf, X_resampled, y_resampled, cv = 5, scoring = ["neg_log_loss", "f1", "precision", "recall"])

lr_cv
print(f"Precision: {lr_cv['test_precision'].mean()}")
print(f"Recall: {lr_cv['test_recall'].mean()}")
print(f"F1: {lr_cv['test_f1'].mean()}")
```

## Get the predicted labels and probabilities from the fitted model using cross_val_predict
```{python}
# Fit the model to the whole training data
lr_clf.fit(X_resampled, y_resampled)
y_pred_lr = cross_val_predict(lr_clf, X_resampled, y_resampled, cv = 5)
y_scores_lr = cross_val_predict(lr_clf, X_resampled, y_resampled, cv = 5, method = "predict_proba")
```

## Visualize the confusion matrix
```{python}
fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_lr, ax = ax)

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_lr, normalize = "true", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by true label values")

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_lr, normalize = "pred", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by predicted label values")

print("Logistic Regression classification report on imbalanced test data")
Markdown(classification_report(y_resampled, y_pred_lr))
```

## Plot the precision recall curve
```{python}
fig, ax = plt.subplots()
plot_precision_recall_curve("Logistic Regression with Balanced Classes", y_resampled, y_scores_lr[:, 1], ax = ax)
```

## Get the most important features 
```{python}
coefficients = np.abs(lr_clf[1].coef_[0])
# # Display feature importance using coefficients and odds ratios
feature_importance_lr = pd.DataFrame({
    'Feature': lr_clf[0].get_feature_names_out(),
    'Coefficient': coefficients
}).sort_values(by='Coefficient', ascending=True)[-20:]
fig, ax = plt.subplots()
feature_importance_lr.plot(kind = "barh", ax = ax, x = "Feature")
ax.set_title("Absolute coefficients of top 20 features in the Logistic Regression Model")
ax.legend("")
ax.set_xlabel("Absolute Value of Coefficient")
plt.show()
fig.savefig("images/lr_feature_importance.png", bbox_inches='tight')
```

## Plot the learning curve for Logistic Regression model
```{python}
cv = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 30) # This is not leading stratified split for some reason.


cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=30)
train_sizes, train_scores, valid_scores = learning_curve(
    lr_clf, X_resampled, y_resampled, train_sizes=np.linspace(0.01, 1.0, 10), cv=cv,
    scoring="f1")
train_sizes, train_scores, valid_scores
```

```{python}
train_errors = train_scores.mean(axis=1)
valid_errors = valid_scores.mean(axis=1)
train_errors, valid_errors
fig, ax = plt.subplots()
ax.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train")
ax.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")
ax.legend()
ax.set_title("Learning curves for Logistic Regression on balanced data (5-fold CV)")
ax.set_ylabel("F1 score")
ax.set_xlabel("Training data size")
plt.show()
fig.savefig("images/lr_learning_curve.png")
```

- The learning curves show that there is still room for some improvement if we increase the data because they have not plateaued yet.
- The learning curves show that there is not much overfitting as the performance on training and validation sets are quite similar when full data are used.

## Save the model
```{python}
import joblib
joblib.dump(lr_clf, "models/diabetes_prediction_lr_model.pkl")
```

# Fit a random forest classifier for comparison
```{python}
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42, n_estimators=100)
rf_clf = Pipeline([
    ("pre_processing", preprocessing),
    ("random_forest", rf)
])

rf_cv = cross_validate(rf_clf, X_resampled, y_resampled, cv=5, scoring=["neg_log_loss", "f1", "precision", "recall"])
rf_cv
print(f"Precision: {rf_cv['test_precision'].mean()}")
print(f"Recall: {rf_cv['test_recall'].mean()}")
print(f"F1: {rf_cv['test_f1'].mean()}")
```

## Plot the learning curve for the Random Forest model

```{python}
cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=30)
train_sizes, train_scores, valid_scores = learning_curve(
    rf_clf, X_resampled, y_resampled, train_sizes=np.linspace(0.01, 1.0, 10), cv=cv,
    scoring="f1")
train_sizes, train_scores, valid_scores
```

```{python}
train_errors = train_scores.mean(axis=1)
valid_errors = valid_scores.mean(axis=1)
train_errors, valid_errors
fig, ax = plt.subplots()
ax.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train")
ax.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")
ax.legend()
ax.set_title("Learning curves for RandomForest on balanced data (5-fold CV)")
ax.set_ylabel("F1 score")
ax.set_xlabel("Training data size")
plt.show()
```

The learning curve hows that the random forest model is overfitting the data. It classifies that training data perfectly and the performance on validation set also plateaus at a very small train size value. The overfitting on the training data shows that the random forest model needs to be regularized.

## Tune the RandomForest model to reduce overfitting

```{python}
param_dist = {
    'random_forest__n_estimators': randint(50, 100),              # Lower tree count to reduce complexity
    'random_forest__max_depth': randint(3, 20),                   # Control tree depth
    'random_forest__min_samples_split': randint(2, 20),           # Don't allow overly small splits
    'random_forest__min_samples_leaf': randint(2, 20),            # Prevent small leaves
    'random_forest__max_features': ['sqrt', 'log2', 0.1, 0.5, 0.8],# Control tree diversity
    'random_forest__bootstrap': [True, False],                    # Bootstrap sampling or full sampling
}

cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=30)

search_rf = RandomizedSearchCV(
    estimator=rf_clf,
    param_distributions=param_dist,
    n_iter=50,                     # Adjust depending on time/resources
    scoring='f1',                  # Or 'roc_auc', 'accuracy', etc.
    cv=cv,
    random_state=42,
    verbose=1
)

search_rf.fit(X_resampled, y_resampled)
best_rf_model = search_rf.best_estimator_
print("Best Parameters:", search_rf.best_params_)
print("Validation Score:", search_rf.best_score_)
```

## Learning curve for the tuned model
```{python}
cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=30)
train_sizes, train_scores, valid_scores = learning_curve(
    best_rf_model, X_resampled, y_resampled, train_sizes=np.linspace(0.01, 1.0, 10), cv=cv,
    scoring="f1")
print(train_sizes, train_scores, valid_scores)
train_errors = train_scores.mean(axis=1)
valid_errors = valid_scores.mean(axis=1)
train_errors, valid_errors
fig, ax = plt.subplots()
ax.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train")
ax.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")
ax.legend()
ax.set_title("Learning curves for RandomForest on balanced data (5-fold CV)")
ax.set_ylabel("F1 score")
ax.set_xlabel("Training data size")
plt.show()
fig.savefig("images/rf_learning_curve.png")
```

Even after tuning, the model random forest model is still overfitting, though the training curve is going down with more data. RandomForest model does not seem to be very useful for these data.

## Get the predicted labels and probabilities from the fitted model using cross_val_predict

```{python}

rf_clf = best_rf_model
rf_clf.fit(X_resampled, y_resampled)

y_pred_rf = cross_val_predict(rf_clf, X_resampled, y_resampled, cv = 5)
y_scores_rf = cross_val_predict(rf_clf, X_resampled, y_resampled, cv = 5, method = "predict_proba")
```


## Visualize the confusion matrix
```{python}
fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_rf, ax = ax)

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_rf, normalize = "true", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by true label values")

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_rf, normalize = "pred", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by predicted label values")

print("Random Forest classification report on imbalanced test data")
Markdown(classification_report(y_resampled, y_pred_rf))
```

## Plot the precision recall curve

```{python}
fig, ax = plt.subplots()
plot_precision_recall_curve("Random Forest with Balanced Classes", y_resampled, y_scores_rf[:, 1], ax = ax)
ax.set_ylim(bottom = 0)
```

## Get the most important features

```{python}
importances = (rf_clf[1].feature_importances_)
# # Display feature importance using coefficients and odds ratios
feature_importance_rf = pd.DataFrame({
    'Feature': rf_clf[0].get_feature_names_out(),
    'Importance': importances
}).sort_values(by='Importance', ascending=True)[-20:]
fig, ax = plt.subplots()
feature_importance_rf.plot(kind = "barh", ax = ax, x = "Feature")
ax.set_title("Absolute coefficients of top 20 features in the Random Forest Model")
ax.legend("")
ax.set_xlabel("Feature Importance Value")
plt.show()
```

# Fit a Support Vector Classifier on the data

```{python}

svc = SVC(probability=True, kernel = "linear",C =10, gamma = "scale", random_state = 40)
svc_clf = Pipeline([
("pre_processing", preprocessing),
("svc", svc)
])

svc_cv = cross_validate(svc_clf, X_resampled, y_resampled, cv = 3, scoring = ["neg_log_loss", "f1", "precision", "recall"])

svc_cv
print(f"Precision: {svc_cv['test_precision'].mean()}")
print(f"Recall: {svc_cv['test_recall'].mean()}")
print(f"F1: {svc_cv['test_f1'].mean()}")
```

## Plot the learning curve for the support vector classifier

```{python}
cv = StratifiedShuffleSplit(n_splits=4, test_size=0.25, random_state=30)
train_sizes, train_scores, valid_scores = learning_curve(
    svc_clf, X_resampled, y_resampled, train_sizes=np.linspace(0.01, 1.0, 5), cv=cv,
    scoring="f1")
train_sizes, train_scores, valid_scores

train_errors = train_scores.mean(axis=1)
valid_errors = valid_scores.mean(axis=1)
train_errors, valid_errors
fig, ax = plt.subplots()
ax.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train")
ax.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")
ax.legend()
ax.set_title("Learning curves for Support Vector Classifier on balanced data (4-fold CV)")
ax.set_ylabel("F1 score")
ax.set_xlabel("Training data size")
plt.show()
fig.savefig("images/svc_learning_curve.png")
```

The learning curves show that the SVC with rbf kernel is also overfitting the training data, though the curves have not plateaued yet and and the training f1 score is falling and validation score is increasing. However, when I ran SVC with linear kernel, the model does not overfit.

## Tune the model?

## Get the predicted labels and probabilities from the fitted model using cross_val_predict

```{python}
svc_clf.fit(X_resampled, y_resampled)
y_pred_svc = cross_val_predict(svc_clf, X_resampled, y_resampled, cv = 3)
y_scores_svc = cross_val_predict(svc_clf, X_resampled, y_resampled, cv = 3, method = "predict_proba")
```

## Visualize the confusion matrix

```{python}
fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_svc, ax = ax)

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_svc, normalize = "true", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by true label values")

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_svc, normalize = "pred", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by predicted label values")

print("Support Vector classification report on imbalanced test data")
Markdown(classification_report(y_resampled, y_pred_svc))
```

## Plot the precision recall curve
```{python}
fig, ax = plt.subplots()
plot_precision_recall_curve("Logistic Regression with Balanced Classes", y_resampled, y_scores_svc[:, 1], ax = ax)
```



# Fit catboost on the data

```{python}
from catboost import CatBoostClassifier


cat_feature_indices = list(range(len(cat_cols_mod)))  # their positions after transformation

ord_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = convert_missing_codes_to_na, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy='most_frequent', add_indicator = False)),
    ("min_max_scaler", MinMaxScaler())
    ])
cat_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = convert_missing_codes_to_na, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy = "most_frequent", add_indicator = False))
])

# ("convert_to_string", FunctionTransformer(to_string, feature_names_out="one-to-one"))
preprocessing = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols_mod),
    ("ord", ord_pipeline, ord_cols_mod)
])

preprocessing.set_output(transform="pandas")




# X_processed.info()
X_processed = preprocessing.fit_transform(X_resampled, y_resampled)
for col in X_processed.columns[cat_feature_indices]:
    X_processed[col] = X_processed[col].astype(str)

X_processed.info()

catboost= CatBoostClassifier(
        cat_features=cat_feature_indices,
        iterations=1000,
        depth=5,
        learning_rate=0.005,
        eval_metric='Logloss',
        early_stopping_rounds=50,
        random_seed=42,
        verbose=100)

cat_cv = cross_validate(catboost, X_processed, y_resampled, cv = 3, scoring = ["neg_log_loss", "f1", "precision", "recall"])

cat_cv

print(f"Precision: {cat_cv['test_precision'].mean()}")
print(f"Recall: {cat_cv['test_recall'].mean()}")
print(f"F1: {cat_cv['test_f1'].mean()}")
```


## Plot the learning curve for catboost
```{python}
cv = StratifiedShuffleSplit(n_splits=4, test_size=0.25, random_state=30)
train_sizes, train_scores, valid_scores = learning_curve(
    catboost, X_processed, y_resampled, train_sizes=np.linspace(0.01, 1.0, 10), cv=cv,
    scoring="f1")
train_sizes, train_scores, valid_scores

train_errors = train_scores.mean(axis=1)
valid_errors = valid_scores.mean(axis=1)
train_errors, valid_errors
fig, ax = plt.subplots()
ax.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train")
ax.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")
ax.legend()
ax.set_title("Learning curves for Catboost on balanced data (5-fold CV)")
ax.set_ylabel("F1 score")
ax.set_xlabel("Training data size")
plt.show()
fig.savefig("images/catboost_learning_curve.png")
```

The learning curve for catboost with a low learning rate slightly overfits the data, but provides a good overall fit.

## Get the predicted labels and probabilities from the fitted model using cross_val_predict

```{python}
catboost.fit(X_processed, y_resampled)
y_pred_cat = cross_val_predict(catboost, X_processed, y_resampled, cv = 5)
y_scores_cat = cross_val_predict(catboost, X_processed, y_resampled, cv = 5, method = "predict_proba")
```

## Visualize the confusion matrix

```{python}
fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_cat, ax = ax)

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_cat, normalize = "true", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by true label values")

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled, y_pred_cat, normalize = "pred", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by predicted label values")

print("CatBoost classification report on balanced test data")
Markdown(classification_report(y_resampled, y_pred_cat))
```

## Plot the precision recall curve

```{python}
fig, ax = plt.subplots()
plot_precision_recall_curve("CatBoost with Balanced Classes", y_resampled, y_scores_cat[:, 1], ax = ax)
ax.set_ylim(bottom = 0)
```

## Get the most important features 

## Save the model
```{python}
import joblib
joblib.dump(catboost, "models/diabetes_prediction_catboost_model.pkl")
```


```{python}
importances = catboost.get_feature_importance()
importances
# # Display feature importance using coefficients and odds ratios
feature_importance_cat = pd.DataFrame({
    'Feature': X_processed.columns,
    'Importance': importances
}).sort_values(by='Importance', ascending=True)[-20:]
fig, ax = plt.subplots()
feature_importance_cat.plot(kind = "barh", ax = ax, x = "Feature")
ax.set_title("Absolute coefficients of top 20 features in the CatBoost Model")
ax.legend("")
ax.set_xlabel("Feature Importance")
plt.show()
fig.savefig("images/catboost_feature_importance.png", bbox_inches='tight')
```




# Most important features in the data
RandomForest Classifier is overfitting the data and SVC is too slow. LogisticRegression and CatBoost as much better. Compare top20 features from LogisticRegression and CatBoost

```{python}
common_important_features = [item for item in feature_importance_cat["Feature"][-10:] if feature_importance_lr["Feature"][-10:].str.contains(item).any()] 
#item = "cat__DHH_SEX"
#feature_importance_lr["Feature"].str.contains(item).any()

#set(feature_importance_cat["Feature"]) & set(feature_importance_lr["Feature"])
print("Common top20 in LogisticRegression and CatBoost:", common_important_features, len(common_important_features))

common_important_feature_names = [item.split("__")[1] for item in common_important_features]
print(common_important_feature_names)
feature_importance_cat, feature_importance_lr
```

- Out of the top 20 features in both algorithms, sixteen features are common
- Out of top 10 features in both algorithms, 8 features are common
- Out of top 5 features in both algorithms, all 5 are common, though the order is slightly different
- Age is the top feature in both algorithms

The 8 features that are common in top 10 in both algorithms are:
1. DHHGAGE - Age
2. CCC_080 - Took medication for high blood cholestrol/lipids in the last one month
3. GEN_005 - Perceived Health - Excellent, very good, good, fair, poor
4. CCC_070 - Took medication for high blood pressure in the last one month
5. ALC_015 - Frequency of drinking alcohol in the last 12 months
6. HWTDGBCC - BMI classification - underweight or overweight/obese
7. DHH_SEX - Sex at birth
8. SDCDVFLA - Are you a visible minority


# Conclusions

We are able to recover many of the known factors associated with diabetes by building a predictive model on Community Health Survery data.
The top factors recovered are age, high blood cholesterol, high blood pressure, BMI, sex, frequency of drinking alcohol, being a minority, and perceived health.

The above features have been recovered in an unbiased way, starting from 691 original features in the survey data.

Predictive machine learning is a powerful tool to extract useful associations between features from surveys, even when data are collected for generic purposes. For example, community health survey data gathers diabetes status as only one of the features, but we were able to recover known lifestyle and health factors associated with diabetes from these data in an unbiased manner.

The predictive models themselves may not be very useful because there is an upper limit to the predictive power because of the fact that all features the remained in the data after pre-processing are discrete. It is also known that features such as blood sugar level are highly predictive of diabetes status, but are absent from the community health survey data.

Therefore, these data can be used for recovering associations, but not to build the best predictive models.


# Check performance on the test dataset
```{python}
y_pred_lr_test = lr_clf.predict(X_test)
```

```{python}
precision = precision_score(y_test, y_pred_lr_test)
recall = recall_score(y_test, y_pred_lr_test)
f1 = 2*precision*recall/(precision+recall)

print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 score: {f1:.2f}")

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_lr_test, ax = ax)

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_lr_test, normalize = "true", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by true label values")

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_lr_test, normalize = "pred", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by predicted label values")

print("Logistic Regression classification report on imbalanced test data")
Markdown(classification_report(y_test, y_pred_lr_test))
```

# Check performance on balanced test data
```{python}
rus = RandomUnderSampler(random_state=0)
X_resampled_test, y_resampled_test = rus.fit_resample(X_test, y_test)
y_pred_lr_resampled_test = lr_clf.predict(X_resampled_test)
```

```{python}
precision = precision_score(y_resampled_test, y_pred_lr_resampled_test)
recall = recall_score(y_resampled_test, y_pred_lr_resampled_test)
f1 = 2*precision*recall/(precision+recall)

print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 score: {f1:.2f}")

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled_test, y_pred_lr_resampled_test, ax = ax)

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled_test, y_pred_lr_resampled_test, normalize = "true", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by true label values")

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_predictions(y_resampled_test, y_pred_lr_resampled_test, normalize = "pred", values_format="0.2%", ax = ax)
ax.set_title("Confusion matrix normalized by predicted label values")

print("Logistic Regression classification report on imbalanced test data")
Markdown(classification_report(y_resampled_test, y_pred_lr_resampled_test))
```