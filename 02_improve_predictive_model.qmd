---
title: "Improve performance of baseline models"
author: Shefali Lathwal
date: "2025-05-16"
date-modified: last-modified
format: html
toc: true
echo: true
jupyter: python3
---

# Import required libraries
```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score
from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from xgboost import XGBClassifier
from sklearn.metrics import ConfusionMatrixDisplay, precision_recall_curve
```

# Import and prepare the data
- The data consist of 108,252 rows and 691 columns.
```{python}
df = pd.read_csv("/Users/shefalilathwal/Documents/diabetes_canada_project/data/pumf_cchs.csv")
print(df.shape)
col_list = df.columns.tolist()
print(col_list)

# Only keeping rows with a known diabetes status
#Codes 9, 7, and 8 are for either refusing to disclose diabetes status, or not knowing or not stated. I should only keep the rows with known diabetes status.

df = df[df["CCC_095"].isin([1.0, 2.0])]
print(df["CCC_095"].value_counts())

# Only keeping rows for adults - removing code = 1
df = df[~(df["DHHGAGE"] == 1)]
print(df["DHHGAGE"].value_counts())

# From manual analysis of the data, I identified important columns and the types of each column
cols_to_keep = ["GEOGPRV", "DHH_SEX", "DHHGMS", "DHHDGHSZ", "DHHGAGE","EHG2DVH3","GENDVHDI", "GENDVMHI", "GENDVSWL","HWTDGISW", "CCC_035", "CCC_065", "CCC_070", "CCC_075", "CCC_080", "CCC_095", "CCC_185", "CCC_195", "CCC_200","CCCDGRSP", "CCCDGSKL", "CCCDGCAR","HUIDGHSI", "HUIDGPAD", "SMKDVSTY", "SMKDGYCS","SMKDGSTP","ETS_005","ALCDVTTM", "ALWDVWKY", "ALWDVDLY","CAN_015","SDS_005","DRGDVYA","DRGDVLA","PAADVTRV", "PAADVREC", "PAADVOTH", "PAADVMVA", "PAADVVIG","PAADVVOL", "LBFDGHPW", "SDC_015","SDCDGCB","SDCDVIMM","SDCDVFLA","INCDGHH"]

ord_cols = ["EHG2DVH3","GENDVHDI","GENDVMHI", "GENDVSWL","SMKDGYCS","SMKDGSTP", "INCDGHH"]
num_cols = ["ALWDVWKY","ALWDVDLY", "PAADVTRV", "PAADVREC","PAADVOTH","PAADVMVA" , "PAADVVIG","PAADVVOL","LBFDGHPW"]
cat_cols = ["GEOGPRV","DHH_SEX", "DHHGMS","DHHDGHSZ","DHHGAGE","HWTDGISW","CCC_035", "CCC_065", "CCC_070", "CCC_075", "CCC_080", "CCC_185", "CCC_195", "CCC_200","CCCDGRSP", "CCCDGSKL", "CCCDGCAR","HUIDGHSI", "HUIDGPAD","SMKDVSTY","ETS_005","ALCDVTTM", "CAN_015", "SDS_005","DRGDVYA","DRGDVLA","SDC_015","SDCDGCB","SDCDVIMM","SDCDVFLA"]
target_col = ["CCC_095"]

print(len(cols_to_keep) == len(ord_cols+num_cols+cat_cols+target_col))

# Only keep the important columns in the data
df = df[cols_to_keep]
print(df.shape)
```

# Define a function to deal with the missing data and remove columns with a large fraction of data missing
- Missing data is encoded as numerical codes, so we define a function to deal with these codes. the function will be used later in data pre-processing.
- I will also use the function to check the missing values in all included columns and remove the ones that have more than a certain percentage of data missing.

```{python}

# Step1:  convert_missing_codes to na

def check_special_missing(df):
    df_cleaned = df.copy()
    missing_summary = {}
    #max_val_list = []
    for col in df.columns:
        #print(col)
        if df[col].dtype in [np.float64, np.int64]:
            unique_vals = df[col].dropna().unique()
            #print(len(unique_vals), unique_vals)
            if len(unique_vals) == 0:
                continue
            max_val = max(unique_vals)
            #max_val_list.append(max_val)
            #print(max_val)
            missing_codes = []
            # Identify the missing value codes based on magnitude
            if max_val < 10:  # single digits (1–9)
                missing_codes = [6, 7, 8, 9]
            elif max_val < 100:  # double digits (01–99)
                missing_codes = [96, 97, 98, 99]
            elif max_val < 1000:  # triple digits (001–999)
                missing_codes = [996, 997, 998, 999, 999.6, 999.7, 999.8, 999.9]
            elif max_val < 10000:
                missing_codes = [9996, 9997, 9998, 9999, 9999.90, 9999.80, 9999.70, 9999.60]
            elif max_val < 100000:
                missing_codes = [99996, 99997, 99998, 99999]
            else:
                continue
            
            # Count and replace
            if missing_codes:
                counts = {code: (df[col] == code).sum() for code in missing_codes}
                if any(counts.values()):
                    missing_summary[col] = counts
                df_cleaned[col] = df_cleaned[col].replace(missing_codes, np.nan)

    # Return cleaned data and summary
    return df_cleaned

cols_with_more_than_60_pct_missing_data = num_cols+["SMKDGYCS", "SMKDGSTP"]+["SDS_005","DRGDVYA","DRGDVLA"]
print(cols_with_more_than_60_pct_missing_data)
cols_with_more_than_30_pct_missing_data = num_cols+["SMKDGYCS", "SMKDGSTP"]+["ETS_005","SDS_005","DRGDVYA","DRGDVLA"]
print(cols_with_more_than_30_pct_missing_data)

# only keep the data from columns that have less than 30% data missing

df = df.drop(columns = cols_with_more_than_30_pct_missing_data, inplace=False)

# Modify variables containing names of categorical and ordinal variables 

ord_cols_mod = [item for item in ord_cols if item not in cols_with_more_than_30_pct_missing_data]
cat_cols_mod = [item for item in cat_cols if item not in cols_with_more_than_30_pct_missing_data]
len(cat_cols_mod), len(cat_cols), len(ord_cols_mod), len(ord_cols)
```

# Split the data into training and testing sets

```{python}
df_train_set, df_test_set = train_test_split(df, test_size = 0.3, random_state = 42, shuffle = True, stratify = df["CCC_095"])

## Confirm stratification
print(df_train_set["CCC_095"].value_counts()/df_train_set.shape[0], df_test_set["CCC_095"].value_counts()/df_test_set.shape[0], df["CCC_095"].value_counts()/df.shape[0])

X_train = df_train_set.copy().drop(columns = "CCC_095", inplace = False)
y_train = df_train_set["CCC_095"].copy()

X_test = df_test_set.copy().drop(columns = "CCC_095", inplace = False)
y_test = df_test_set["CCC_095"].copy()


# Convert the target variable into True and False
y_train = (y_train == 1)
y_test = (y_test == 1)
y_train.sum(), y_test.sum(), y_train, y_test

```

# Define the pre-processing pipeline

```{python}
ord_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = check_special_missing, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy='most_frequent', add_indicator = False)),
    ("min_max_scaler", MinMaxScaler())
    ])

cat_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = check_special_missing, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy = "most_frequent", add_indicator = False)),
    ("one_hot_encoder", OneHotEncoder(sparse_output=False, handle_unknown="ignore"))
])

preprocessing = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols_mod),
    ("ord", ord_pipeline, ord_cols_mod)
])
```

# Run XGBoost on the data
```{python}

# XGBOOST CLASSIFIER
xgb_clf = Pipeline([
    ('pre_processing', preprocessing),
    ('classifier', XGBClassifier(
        scale_pos_weight=(y_train == False).sum() / (y_train == True).sum(),  # handles imbalance
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=42
    ))
])

#xgb_pipeline.fit(X_train, y_train)
```

```{python}
xgb_clf.fit(X_train, y_train)

print(xgb_clf)
```

```{python}
cross_val_score(xgb_clf, X_train, y_train, scoring='f1', cv=3)
```

```{python}
y_pred = cross_val_predict(xgb_clf, X_train, y_train, cv = 3, method = "predict_proba")
y_pred
```

# Visualize results in a confusion matrix

```{python}
y_prob = (y_pred[:, 1] >=0.5)
y_prob

ConfusionMatrixDisplay.from_predictions(y_train, y_prob, display_labels=xgb_clf.classes_)
```

# Plot the precision/recall curve

```{python}
y_probs = y_pred[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_train, y_probs)
plt.plot(precisions, recalls)
plt.xlabel("Precision")
plt.ylabel("Recall")
plt.grid()
plt.legend("")
```


# Notes:
- Running XGBClassifier inside a pipeline and inside the function cross_val_score was giving an error for a long time. In the end, I upgraded the versions of xgboost and scikit-learn and the error disappeared. However, the performance of the XGBClassifier is same as logistic regression and SVC, an f1 score of 0.39