---
title: "Improve performance of baseline models"
author: Shefali Lathwal
date: "2025-05-16"
date-modified: last-modified
format: html
toc: true
echo: true
jupyter: python3
---

# Import required libraries
```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score
from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from xgboost import XGBClassifier
from sklearn.metrics import ConfusionMatrixDisplay, precision_recall_curve
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
#from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
#from sklearn.ensemble import StackingClassifier
from sklearn.base import clone
import seaborn as sns
from sklearn.feature_selection import VarianceThreshold

```

# Import and prepare the data
- The data consist of 108,252 rows and 691 columns.
```{python}
df = pd.read_csv("/Users/shefalilathwal/Documents/diabetes_canada_project/data/pumf_cchs.csv")
print(df.shape)
col_list = df.columns.tolist()
print(col_list)

# Only keeping rows with a known diabetes status
#Codes 9, 7, and 8 are for either refusing to disclose diabetes status, or not knowing or not stated. I should only keep the rows with known diabetes status.

df = df[df["CCC_095"].isin([1.0, 2.0])]
print(df["CCC_095"].value_counts())

# Only keeping rows for adults - removing code = 1
df = df[~(df["DHHGAGE"] == 1)]
print(df["DHHGAGE"].value_counts())

# From manual analysis of the data, I identified important columns and the types of each column
cols_to_keep = ["GEOGPRV", "DHH_SEX", "DHHGMS", "DHHDGHSZ", "DHHGAGE","EHG2DVH3","GENDVHDI", "GENDVMHI", "GENDVSWL","HWTDGISW", "CCC_035", "CCC_065", "CCC_070", "CCC_075", "CCC_080", "CCC_095", "CCC_185", "CCC_195", "CCC_200","CCCDGRSP", "CCCDGSKL", "CCCDGCAR","HUIDGHSI", "HUIDGPAD", "SMKDVSTY", "SMKDGYCS","SMKDGSTP","ETS_005","ALCDVTTM", "ALWDVWKY", "ALWDVDLY","CAN_015","SDS_005","DRGDVYA","DRGDVLA","PAADVTRV", "PAADVREC", "PAADVOTH", "PAADVMVA", "PAADVVIG","PAADVVOL", "LBFDGHPW", "SDC_015","SDCDGCB","SDCDVIMM","SDCDVFLA","INCDGHH"]

ord_cols = ["EHG2DVH3","GENDVHDI","GENDVMHI", "GENDVSWL","SMKDGYCS","SMKDGSTP", "INCDGHH"]
num_cols = ["ALWDVWKY","ALWDVDLY", "PAADVTRV", "PAADVREC","PAADVOTH","PAADVMVA" , "PAADVVIG","PAADVVOL","LBFDGHPW"]
cat_cols = ["GEOGPRV","DHH_SEX", "DHHGMS","DHHDGHSZ","DHHGAGE","HWTDGISW","CCC_035", "CCC_065", "CCC_070", "CCC_075", "CCC_080", "CCC_185", "CCC_195", "CCC_200","CCCDGRSP", "CCCDGSKL", "CCCDGCAR","HUIDGHSI", "HUIDGPAD","SMKDVSTY","ETS_005","ALCDVTTM", "CAN_015", "SDS_005","DRGDVYA","DRGDVLA","SDC_015","SDCDGCB","SDCDVIMM","SDCDVFLA"]
target_col = ["CCC_095"]

print(len(cols_to_keep) == len(ord_cols+num_cols+cat_cols+target_col))

# Only keep the important columns in the data
df = df[cols_to_keep]
print(df.shape)
```

# Define a function to deal with the missing data and remove columns with a large fraction of data missing
- Missing data is encoded as numerical codes, so we define a function to deal with these codes. the function will be used later in data pre-processing.
- I will also use the function to check the missing values in all included columns and remove the ones that have more than a certain percentage of data missing.

```{python}

# Step1:  convert_missing_codes to na

def check_special_missing(df):
    df_cleaned = df.copy()
    missing_summary = {}
    #max_val_list = []
    for col in df.columns:
        #print(col)
        if df[col].dtype in [np.float64, np.int64]:
            unique_vals = df[col].dropna().unique()
            #print(len(unique_vals), unique_vals)
            if len(unique_vals) == 0:
                continue
            max_val = max(unique_vals)
            #max_val_list.append(max_val)
            #print(max_val)
            missing_codes = []
            # Identify the missing value codes based on magnitude
            if max_val < 10:  # single digits (1â€“9)
                missing_codes = [6, 7, 8, 9]
            elif max_val < 100:  # double digits (01â€“99)
                missing_codes = [96, 97, 98, 99]
            elif max_val < 1000:  # triple digits (001â€“999)
                missing_codes = [996, 997, 998, 999, 999.6, 999.7, 999.8, 999.9]
            elif max_val < 10000:
                missing_codes = [9996, 9997, 9998, 9999, 9999.90, 9999.80, 9999.70, 9999.60]
            elif max_val < 100000:
                missing_codes = [99996, 99997, 99998, 99999]
            else:
                continue
            
            # Count and replace
            if missing_codes:
                counts = {code: (df[col] == code).sum() for code in missing_codes}
                if any(counts.values()):
                    missing_summary[col] = counts
                df_cleaned[col] = df_cleaned[col].replace(missing_codes, np.nan)

    # Return cleaned data and summary
    return df_cleaned

cols_with_more_than_60_pct_missing_data = num_cols+["SMKDGYCS", "SMKDGSTP"]+["SDS_005","DRGDVYA","DRGDVLA"]
print(cols_with_more_than_60_pct_missing_data)
cols_with_more_than_30_pct_missing_data = num_cols+["SMKDGYCS", "SMKDGSTP"]+["ETS_005","SDS_005","DRGDVYA","DRGDVLA"]
print(cols_with_more_than_30_pct_missing_data)

# only keep the data from columns that have less than 30% data missing

df = df.drop(columns = cols_with_more_than_30_pct_missing_data, inplace=False)

# Modify variables containing names of categorical and ordinal variables 

ord_cols_mod = [item for item in ord_cols if item not in cols_with_more_than_30_pct_missing_data]
cat_cols_mod = [item for item in cat_cols if item not in cols_with_more_than_30_pct_missing_data]
len(cat_cols_mod), len(cat_cols), len(ord_cols_mod), len(ord_cols)
```

# Split the data into training and testing sets

```{python}
df_train_set, df_test_set = train_test_split(df, test_size = 0.3, random_state = 42, shuffle = True, stratify = df["CCC_095"])

## Confirm stratification
print(df_train_set["CCC_095"].value_counts()/df_train_set.shape[0], df_test_set["CCC_095"].value_counts()/df_test_set.shape[0], df["CCC_095"].value_counts()/df.shape[0])

X_train = df_train_set.copy().drop(columns = "CCC_095", inplace = False)
y_train = df_train_set["CCC_095"].copy()

X_test = df_test_set.copy().drop(columns = "CCC_095", inplace = False)
y_test = df_test_set["CCC_095"].copy()


# Convert the target variable into True and False
y_train = (y_train == 1)
y_test = (y_test == 1)
y_train.sum(), y_test.sum(), y_train, y_test

```

# Define the pre-processing pipeline

```{python}
ord_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = check_special_missing, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy='most_frequent', add_indicator = False)),
    ("min_max_scaler", MinMaxScaler())
    ])

cat_pipeline = Pipeline([
    ("convert_to_na", FunctionTransformer(func = check_special_missing, feature_names_out="one-to-one")),
    ("fill_na", SimpleImputer(strategy = "most_frequent", add_indicator = False)),
    ("one_hot_encoder", OneHotEncoder(sparse_output=False, handle_unknown="ignore", drop = "first"))
])

preprocessing = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols_mod),
    ("ord", ord_pipeline, ord_cols_mod)
])
```

# Run XGBoost on the data
```{python}

# XGBOOST CLASSIFIER
xgb_clf = Pipeline([
    ('pre_processing', preprocessing),
    ('classifier', XGBClassifier(
        scale_pos_weight=(y_train == False).sum() / (y_train == True).sum(),  # handles imbalance
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=42
    ))
])

#xgb_pipeline.fit(X_train, y_train)
```

```{python}
xgb_clf.fit(X_train, y_train)

print(xgb_clf)
```

```{python}
cross_val_score(xgb_clf, X_train, y_train, scoring='f1', cv=3)
```

```{python}
y_pred = cross_val_predict(xgb_clf, X_train, y_train, cv = 3, method = "predict_proba")
y_pred
```

# Visualize results in a confusion matrix

```{python}
y_prob = (y_pred[:, 1] >=0.5)
y_prob

ConfusionMatrixDisplay.from_predictions(y_train, y_prob, display_labels=xgb_clf.classes_)
```

# Plot the precision/recall curve

```{python}
y_probs = y_pred[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_train, y_probs)
plt.plot(precisions, recalls)
plt.xlabel("Precision")
plt.ylabel("Recall")
plt.grid()
plt.legend("")
```


# Notes:
- Running XGBClassifier inside a pipeline and inside the function cross_val_score was giving an error for a long time. In the end, I upgraded the versions of xgboost and scikit-learn and the error disappeared. However, the performance of the XGBClassifier is same as logistic regression and SVC, an f1 score of 0.39


# Try tuning the XgBoost algorithm

```{python}
# Define parameter grid for XGBoost
xgb_params = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [3, 5, 7],
    'classifier__learning_rate': [0.01, 0.1, 0.2],
    'classifier__subsample': [0.7, 0.8, 1.0],
    'classifier__colsample_bytree': [0.7, 0.8, 1.0]
}

xgb_search = RandomizedSearchCV(
    xgb_clf,
    xgb_params,
    n_iter=20,
    cv=5,
    scoring='f1',
    verbose=1,
    random_state=42,
    n_jobs=-1
)

xgb_search.fit(X_train, y_train)

print("ðŸ”§ Best XGBoost Parameters:")
print(xgb_search.best_params_)

# Evaluate best model
xgb_best = xgb_search.best_estimator_

```

```{python}
xgb_best
```

```{python}
y_pred_xgb_best = cross_val_predict(xgb_best, X_train, y_train, cv=3)
print("F1 Score:", f1_score(y_train, y_pred_xgb_best))
print(classification_report(y_train, y_pred_xgb_best))

y_pred = cross_val_predict(xgb_best, X_train, y_train, cv = 3, method = "predict_proba")
y_pred

y_probs = y_pred[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_train, y_probs)
plt.plot(precisions, recalls)
plt.xlabel("Precision")
plt.ylabel("Recall")
plt.grid()
plt.legend("")
```


# Tune Logistic Regression
```{python}
# 2. Logistic Regression
lr = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)

log_reg = Pipeline([
("pre_processing", preprocessing),
("logistic_regression", lr)
])

lr_params = {
    'logistic_regression__penalty': ['l1', 'l2'],
    'logistic_regression__C': [0.01, 0.1, 1, 10]
}
lr_search = RandomizedSearchCV(log_reg, lr_params, scoring='f1', n_iter=8, cv=5, random_state=42, n_jobs=-1)
lr_search.fit(X_train, y_train)
```

```{python}
log_reg_best = lr_search.best_estimator_
print(lr_search.best_params_)
cv_res = pd.DataFrame(lr_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_res
```

```{python}
y_score = cross_val_predict(log_reg_best, X_train, y_train, cv=5, method = "predict_proba")
y_score
```

```{python}
#y_pred = (y_score[:, 1] > 0.5)
precisions, recalls, thresholds = precision_recall_curve(y_train, y_score[:, 1])
plt.plot(precisions, recalls)
plt.xlabel("Precision")
plt.ylabel("Recall")
plt.grid()
plt.legend("")
```

- Tuning the linear regression model, it is clear that there is not much improvement in the model. All f1 scores are close to 0.39

# Try tuning the random forest model

```{python}
rf = RandomForestClassifier(class_weight='balanced', random_state=42)
forest_clf = Pipeline([
("pre_processing", preprocessing),
("classifier", rf)
])


rf_params = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [5, 10, 20, None],
    'classifier__min_samples_split': [2, 5],
    'classifier__min_samples_leaf': [1, 2, 4]
}
rf_search = RandomizedSearchCV(forest_clf, rf_params, scoring='f1', n_iter=10, cv=5, random_state=42, n_jobs=-1)
rf_search.fit(X_train, y_train)
```

```{python}
forest_clf_best = rf_search.best_estimator_
print(rf_search.best_params_)
```

```{python}
forest_clf_best = rf_search.best_estimator_
print(rf_search.best_params_)
cv_res = pd.DataFrame(rf_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_res
```

The performance of RandomForest improved a lot with tuning! increasing from 0.13 to 0.42

```{python}
y_score = cross_val_predict(forest_clf_best, X_train, y_train, cv=5, method = "predict_proba")
y_score
```

```{python}
precisions, recalls, thresholds = precision_recall_curve(y_train, y_score[:, 1])
plt.plot(precisions, recalls)
plt.xlabel("Precision")
plt.ylabel("Recall")
plt.grid()
plt.legend("")
```

The precision recall curve does not look great, but let's look at the confusion matrix
```{python}
ConfusionMatrixDisplay.from_predictions(y_train, (y_score[:,1]>0.5), display_labels=forest_clf_best[1].classes_)
print(classification_report(y_train, (y_score[:,1]>0.5)))
```

Look at which features are the most important in the random forest algorithm
```{python}
feature_importances = forest_clf_best[1].feature_importances_
feature_importances
for item in sorted(zip(feature_importances, forest_clf_best[0].get_feature_names_out()), reverse = True):
    print(item)
```

# Try removing unimportant features

Best parameters for random_forest are as follows:
{'classifier__n_estimators': 300,
 'classifier__min_samples_split': 2,
 'classifier__min_samples_leaf': 4,
 'classifier__max_depth': 20}
```{python}
#rf = RandomForestClassifier(class_weight='balanced', random_state=42,
#n_estimators=300, min_samples_split=2, min_samples_leaf=4,max_depth=20)
#forest_clf = Pipeline([
#("pre_processing", preprocessing),
#("classifier", rf)
#])
forest_clf_best = rf_search.best_estimator_

importances = pd.Series(forest_clf_best[1].feature_importances_, index=forest_clf_best[0].get_feature_names_out())
importances.sort_values(ascending=False).plot(kind='bar', figsize=(12, 6))
plt.title("Feature Importances from Random Forest")
plt.show()
```

## Reduce the data
```{python}
threshold = 0.01
important_features = importances[importances > threshold].index
important_features
# Identify which original features to keep
#important_ohe_features = [f for f in important_features if f in ohe_feature_names]
#important_numerical_features = [f for f in important_features if f in numerical_cols]

# Map OHE features back to original categorical columns
#important_cols = set()
for f in important_features:
    print(f)
    original_col = f.split('__')[1]
    print(original_col)
    #important_cols.add(original_col)
important_cols = ["GEOGPRV","DHH_SEX","DHHGMS","DHHDGHSZ","DHHGAGE","HWTDGISW","CCC_065","CCC_070","CCC_075","CCC_080","CCCDGSKL","CCCDGCAR","SMKDVSTY","ALCDVTTM","EHG2DVH3","GENDVHDI","GENDVMHI","INCDGHH","CCC_035","HUIDGHSI","HUIDGPAD","CAN_015","GENDVSWL"]

X_train_reduced = X_train[important_cols]
X_test_reduced = X_test[important_cols]
```

```{python}
X_train_reduced.columns.tolist(), y_train
ord_cols_reduced = [item for item in ord_cols_mod if item in important_cols]
cat_cols_reduced = [item for item in cat_cols_mod if item in important_cols]
```

## Retrain model on reduced data
```{python}
reduced_preprocessing_pipeline = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols_reduced),
    ("ord", ord_pipeline, ord_cols_reduced)
])

rf = RandomForestClassifier(class_weight='balanced', random_state=42)
forest_clf_reduced = Pipeline([
    ("pre_processing", reduced_preprocessing_pipeline),
    ("classifier", rf)
])
```

```{python}
print("Original number of features:", X_train.shape[1])
print("Reduced number of features:", X_train_reduced.shape[1])

rf_params = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [5, 10, 20, None],
    'classifier__min_samples_split': [2, 5],
    'classifier__min_samples_leaf': [1, 2, 4]
}
rf_reduced_search = RandomizedSearchCV(forest_clf_reduced, rf_params, scoring='f1', n_iter=10, cv=5, random_state=42, n_jobs=-1)
rf_reduced_search.fit(X_train_reduced, y_train)

```

```{python}
forest_clf_reduced_best = rf_reduced_search.best_estimator_
print(rf_reduced_search.best_params_)
cv_res = pd.DataFrame(rf_reduced_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_res
```

After dropping the additional variables, I am still getting an f1 score of 0.42

# Re-tune the RandomForest model
Best parameters are: 
{'classifier__n_estimators': 300,
 'classifier__min_samples_split': 2,
 'classifier__min_samples_leaf': 4,
 'classifier__max_depth': 20}

 Out of the chosen parameters, 3 are the maximum values given. Therefore, I should try more values for those.




```{python}
rf = RandomForestClassifier(class_weight='balanced', random_state=42)
forest_clf = Pipeline([
("pre_processing", preprocessing),
("classifier", rf)
])


rf_params_2 = {
    'classifier__n_estimators': [300, 400, 500],
    'classifier__max_depth': [20,25,30,40],
    'classifier__min_samples_split': [2,5],
    'classifier__min_samples_leaf': [4, 6, 8]
}



rf_search_2 = RandomizedSearchCV(forest_clf, rf_params_2, scoring='f1', n_iter=10, cv=5, random_state=42, n_jobs=-1)
rf_search_2.fit(X_train, y_train)
```

```{python}
forest_clf_best = rf_search_2.best_estimator_
print(rf_search_2.best_params_)
cv_res = pd.DataFrame(rf_search_2.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_res
```

 New ones came out to be the following:
 {'classifier__n_estimators': 300, 'classifier__min_samples_split': 2, 'classifier__min_samples_leaf': 4, 'classifier__max_depth': 40}
Try tuning a 3rd time
```{python}
rf_params_3 = {
    'classifier__n_estimators': [300],
    'classifier__max_depth': [40, 60, 80, 100],
    'classifier__min_samples_split': [2,4],
    'classifier__min_samples_leaf': [4, 6, 8]
}
rf_search_3 = RandomizedSearchCV(forest_clf, rf_params_3, scoring='f1', n_iter=10, cv=5, random_state=42, n_jobs=-1)
rf_search_3.fit(X_train, y_train)
```

```{python}
forest_clf_best = rf_search_3.best_estimator_
print(rf_search_3.best_params_)
cv_res = pd.DataFrame(rf_search_3.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_res
```

Final params:

{'classifier__n_estimators': 300, 'classifier__min_samples_split': 2, 'classifier__min_samples_leaf': 4, 'classifier__max_depth': 40}


# Is the way i am doing on-hot encoding wrong? If My column has two variables, I should get only one one-hot encoded column, but I am getting two.
- According to introduction to statistical learning - yes. I should keep only one value for a categorical column with two values. This can be done by setting the argument drop = True in OneHotEncoder()
- After I fixed the one-hot encoding, the final f1 score did not improve much, but I am getting a few more columns in the important columns list.


# Try One-hot Encoding for all variables, even the ordinal ones
```{python}
cat_preprocessing_pipeline = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols_mod+ord_cols_mod)
])

rf = RandomForestClassifier(class_weight='balanced', random_state=42)
forest_clf_cat = Pipeline([
    ("pre_processing", cat_preprocessing_pipeline),
    ("classifier", rf)
])
```

```{python}
rf_params = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [5, 10, 20, None],
    'classifier__min_samples_split': [2, 5],
    'classifier__min_samples_leaf': [1, 2, 4]
}
rf_cat_search = RandomizedSearchCV(forest_clf_cat, rf_params, scoring='f1', n_iter=10, cv=5, random_state=42, n_jobs=-1)
rf_cat_search.fit(X_train, y_train)

```


```{python}
forest_clf_cat_best = rf_cat_search.best_estimator_
print(rf_cat_search.best_params_)
cv_res = pd.DataFrame(rf_cat_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_res
```

```{python}
importances = pd.Series(forest_clf_cat_best[1].feature_importances_, index=forest_clf_cat_best[0].get_feature_names_out())
importances.sort_values(ascending=True).plot(kind='barh', figsize=(6, 12))
plt.title("Feature Importances from Random Forest")
plt.show()
```

# Examine features more closely

## Check for correlated features
Since my features are categorical, pearson is not the right metric to use.
- I should use spearman rank correlation for ordinal variables
- For categorical variables, chtGPT is suggesting something else:
```{python}
from scipy.stats import chi2_contingency


# For ordinal variables
corr_matrix = X_train[ord_cols_mod].corr(method = "spearman")
corr_matrix

fig, ax = plt.subplots(figsize=(6,6))
sns.heatmap(corr_matrix, annot = True, cmap = "coolwarm", ax= ax)

# For categorical/nominal variables
# Simple label encoding just for correlation (not for model)
df_encoded = df.copy()
for col in cat_cols_mod:
    df_encoded[col] = df[col].astype('category').cat.codes

def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    return np.sqrt(phi2 / min(k - 1, r - 1))

cramers_results = pd.DataFrame(index=cat_cols_mod, columns=cat_cols_mod)

for col1 in cat_cols_mod:
    for col2 in cat_cols_mod:
        if col1 == col2:
            cramers_results.loc[col1, col2] = 1.0
        else:
            val = cramers_v(df[col1], df[col2])
            cramers_results.loc[col1, col2] = round(val, 3)
#print("\nCramÃ©r's V (Nominal Features):")
#print(cramers_results)

fig, ax = plt.subplots(figsize=(20,20))
sns.heatmap(cramers_results.astype(float), ax = ax, annot = True, cmap = "coolwarm")

def find_high_correlations(corr_matrix, threshold=0.8):
    correlated_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                col1 = corr_matrix.columns[i]
                col2 = corr_matrix.columns[j]
                correlated_pairs.append((col1, col2, corr_matrix.iloc[i, j]))
    return correlated_pairs


print("\nHighly correlated nominal features (CramÃ©r's V > 0.8):")
print(find_high_correlations(cramers_results.astype(float), threshold = 0.5))

```



# Check features with very low variance

- There are no low variance columns in the data
```{python}


selector = VarianceThreshold(threshold=0.01)
selector.fit(X_train)
selector.get_support()
low_variance_cols = X_train.columns[~selector.get_support()]
low_variance_cols
```

# Try stacking classifiers - Not working right now
```{python}


stacking_clf = StackingClassifier(
    estimators=[
        ('rf', rf_search.best_estimator_),
        ('lr', lr_search.best_estimator_),
        ('xgb', xgb_search.best_estimator_)
    ],
    final_estimator=LogisticRegression(),
    n_jobs=1
)

original_columns = X_train.columns.tolist()
def make_dataframe_transformer(column_names):
    def ensure_dataframe(X):
        import pandas as pd
        if isinstance(X, pd.DataFrame):
            return X
        else:
            return pd.DataFrame(X, columns=column_names)
    return FunctionTransformer(ensure_dataframe, feature_names_out="one-to-one")
df_transformer = make_dataframe_transformer(original_columns)

stacking_pipeline = Pipeline([
    ("ensure_df", df_transformer),
    ("preprocessing", preprocessing),
    ("classifier", stacking_clf)
])

#stacking_pipeline = Pipeline([
#    ("pre_processing", preprocessing),
#    ("classifier", stacking_clf)
#])

```

```{python}
stacking_score = cross_val_score(stacking_pipeline, X_train, y_train, cv = 3, scoring = "f1", n_jobs=1) # n_jobs = 1 is to serialize and avoid parallelization
stacking_score
```

```{python}
y_pred_stack = cross_val_predict(stacking_pipeline, X_train, y_train, cv = 3)
print("\nðŸ”— Stacking Classifier Performance:")
print("F1 Score:", f1_score(y_test, y_pred_stack))
print(classification_report(y_test, y_pred_stack))
```




# Try resampling - Not Working
- Resampling strategy with SMOTE is leading to crash of the jupyter kernel and is not a good idea.
